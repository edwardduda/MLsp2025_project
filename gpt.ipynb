{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, Union, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip, ToImage, ToDtype, Normalize\n",
    ")\n",
    "from torchvision.utils import make_grid\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸  Using device: mps\n"
     ]
    }
   ],
   "source": [
    "SEED               = 42\n",
    "N_EPOCHS           = 20\n",
    "BATCH_SIZE         = 128\n",
    "IMAGE_SIZE         = (64, 64)            # Miniâ€‘ImageNet default â†’ 84Ã—84\n",
    "LEARNING_RATE      = 3e-4\n",
    "WEIGHT_DECAY       = 1e-4\n",
    "RUN_DIR            = Path(\"runs/miniimagenet_cnn_vs_kan\").resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"âž¡ï¸  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸  Dataset ready: 45000 train samples, 5000 val, 10000 test | 200 classes\n"
     ]
    }
   ],
   "source": [
    "def _force_rgb(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure 3â€‘channel RGB.\n",
    "\n",
    "    â€¢ If RGBA âžœ drop alpha.\n",
    "    â€¢ If grayscale âžœ replicate to 3 channels.\n",
    "    \"\"\"\n",
    "    if img.shape[0] == 4:\n",
    "        print(\"RGBA\")# RGBA â†’ RGB\n",
    "        return img[:3]\n",
    "    if img.shape[0] == 1:       # Gray â†’ RGB by tiling\n",
    "        return img.repeat(3, 1, 1)\n",
    "    return img  # already RGB\n",
    "\n",
    "def get_dataloaders(batch_size:int=BATCH_SIZE,\n",
    "                    image_size:Tuple[int,int]=IMAGE_SIZE,\n",
    "                    dataset_name:str=\"zh-plus/tiny-imagenet\"):\n",
    "    \"\"\"\n",
    "    Return train/val/test DataLoaders for Miniâ€‘ImageNet.\n",
    "\n",
    "    dataset_name:\n",
    "        â€¢ 'timm/mini-imagenet' â€“ 60â€¯000 images, 200 classes\n",
    "        â€¢ 'zh-plus/tiny-imagenet' â€“ smaller, good for quick prototyping\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    train_ds = ds[\"train\"].train_test_split(\n",
    "        test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n",
    "    val_ds   = train_ds[\"test\"]\n",
    "    train_ds = train_ds[\"train\"]\n",
    "    test_ds  = ds[\"validation\"]\n",
    "\n",
    "    # Normalisation stats from ImageNet\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    tfms = Compose([\n",
    "        ToImage(),                       # PIL â†’ (C,H,W) uint8 tensor\n",
    "        Resize(image_size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    def add_tfms(example):\n",
    "        example[\"image\"] = tfms(example[\"image\"])\n",
    "        return example\n",
    "\n",
    "    for split in (train_ds, val_ds, test_ds):\n",
    "        split.set_transform(add_tfms)\n",
    "\n",
    "    loader_cfg = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "    val_loader   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "    test_loader  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "print(f\"âž¡ï¸  Dataset ready: {len(train_loader.dataset)} train samples, \"\n",
    "      f\"{len(val_loader.dataset)} val, {len(test_loader.dataset)} test | \"\n",
    "      f\"200 classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"A lightweight CNN with two conv blocks + MLP head.\"\"\"\n",
    "    def __init__(self, input_shape=(4, *IMAGE_SIZE),\n",
    "                 conv_channels=(64, 128), fc_dims=(512,)):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2 = conv_channels\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(C_in, c1, 3, padding=1, bias=False), nn.BatchNorm2d(c1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(c1, c2, 3, padding=1, bias=False), nn.BatchNorm2d(c2), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            flat = self.features(torch.zeros(1, *input_shape)).view(1, -1).shape[1]\n",
    "        mlp: List[nn.Module] = []\n",
    "        in_dim = flat\n",
    "        for h in fc_dims:\n",
    "            mlp += [nn.Linear(in_dim, h), nn.ReLU(inplace=True)]\n",
    "            in_dim = h\n",
    "        mlp.append(nn.Linear(in_dim, 200))\n",
    "        self.classifier = nn.Sequential(*mlp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.BSplineActivation import BSplineActivation\n",
    "class KANCNN(nn.Module):\n",
    "    \"\"\"CNN backbone identical to baseline but MLP replaced by KAN.\"\"\"\n",
    "    def __init__(self, input_shape=(3, *IMAGE_SIZE),\n",
    "                 conv_channels=(64, 128), kan_inner=128, kan_outer=200,\n",
    "                 spline_cp=7, spline_deg=2):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2 = conv_channels\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(C_in, c1, 3, padding=1, bias=False), nn.BatchNorm2d(c1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(c1, c2, 3, padding=1, bias=False), nn.BatchNorm2d(c2), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            flat = self.features(torch.zeros(1, *input_shape)).view(1, -1).shape[1]\n",
    "        self.inner = nn.Linear(flat, kan_inner)\n",
    "        self.inner_act = BSplineActivation(num_control_points=spline_cp, degree=spline_deg, range_min=-1, range_max=1)\n",
    "        self.outer = nn.Linear(kan_inner, kan_outer)\n",
    "        self.outer_act = BSplineActivation(num_control_points=spline_cp, degree=spline_deg, range_min=-1, range_max=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner_act(self.inner(x))\n",
    "        x = self.outer_act(self.outer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Train {epoch:02d}\", leave=False):\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item()  * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item() * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "\n",
    "def run_training(model:nn.Module, name:str,\n",
    "                 train_loader:DataLoader, val_loader:DataLoader) -> Dict[str, List[float]]:\n",
    "    \"\"\"Full training loop for one model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        tic = time.time()\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "        val_metrics   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch:2d}/{N_EPOCHS} â€¢ \"\n",
    "              f\"train acc {train_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"val acc {val_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"Î”t {time.time()-tic:4.1f}s\")\n",
    "\n",
    "    torch.save(model.state_dict(), RUN_DIR/f\"{name}.pt\")\n",
    "    with open(RUN_DIR/f\"{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df:pd.DataFrame):\n",
    "    \"\"\"Plot accuracy + loss curves for both models.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_acc\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_acc\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_acc\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_acc\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Miniâ€‘ImageNet â€¢ Accuracy vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"accuracy_curves.png\", dpi=200)\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_loss\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_loss\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_loss\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_loss\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Crossâ€‘entropy loss\")\n",
    "    ax.set_title(\"Miniâ€‘ImageNet â€¢ Loss vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"loss_curves.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Training baseline CNN â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 4, 3, 3], expected input[128, 3, 84, 84] to have 4 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Test accuracy: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBaseline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_base[\u001b[33m'\u001b[39m\u001b[33macc\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKAN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_kan[\u001b[33m'\u001b[39m\u001b[33macc\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m kan      = KANCNN()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“š Training baseline CNN â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m hist_base = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbaseline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸŒ€ Training KAN-CNN â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m hist_kan  = run_training(kan,      \u001b[33m\"\u001b[39m\u001b[33mKAN\u001b[39m\u001b[33m\"\u001b[39m,      train_loader, val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model, name, train_loader, val_loader)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, N_EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m     52\u001b[39m     tic = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     train_metrics = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     val_metrics   = evaluate(model, val_loader, criterion)\n\u001b[32m     56\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_metrics[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     10\u001b[39m y = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m loss = criterion(logits, y)\n\u001b[32m     15\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mBaselineCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classifier(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [64, 4, 3, 3], expected input[128, 3, 84, 84] to have 4 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    baseline = BaselineCNN()\n",
    "    kan      = KANCNN()\n",
    "\n",
    "    print(\"ðŸ“š Training baseline CNN â€¦\")\n",
    "    hist_base = run_training(baseline, \"baseline\", train_loader, val_loader)\n",
    "\n",
    "    print(\"\\nðŸŒ€ Training KAN-CNN â€¦\")\n",
    "    hist_kan  = run_training(kan,      \"KAN\",      train_loader, val_loader)\n",
    "\n",
    "    # convert to DataFrame for plotting\n",
    "    df = pd.DataFrame({\n",
    "        \"epoch\": range(1, N_EPOCHS+1),\n",
    "        \"baseline_train_acc\": [x*100 for x in hist_base[\"train_acc\"]],\n",
    "        \"baseline_val_acc\"  : [x*100 for x in hist_base[\"val_acc\"]],\n",
    "        \"baseline_train_loss\": hist_base[\"train_loss\"],\n",
    "        \"baseline_val_loss\"  : hist_base[\"val_loss\"],\n",
    "        \"kan_train_acc\":      [x*100 for x in hist_kan[\"train_acc\"]],\n",
    "        \"kan_val_acc\":        [x*100 for x in hist_kan[\"val_acc\"]],\n",
    "        \"kan_train_loss\":     hist_kan[\"train_loss\"],\n",
    "        \"kan_val_loss\":       hist_kan[\"val_loss\"],\n",
    "    })\n",
    "    plot_metrics(df)\n",
    "\n",
    "    # Final test evaluation\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_base = evaluate(baseline, test_loader, criterion)\n",
    "    test_kan  = evaluate(kan,      test_loader, criterion)\n",
    "\n",
    "    print(f\"\\nâœ… Test accuracy: \"\n",
    "          f\"Baseline {test_base['acc']*100:5.2f}% | \"\n",
    "          f\"KAN {test_kan['acc']*100:5.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
