{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, Union, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip, ToImage,\n",
    "    ToDtype, Normalize, Lambda                    # ⬅ new\n",
    ")\n",
    "from torchvision.utils import make_grid\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# -----------------------  constants  ---------------------------------\n",
    "SEED               = 42\n",
    "N_EPOCHS           = 44\n",
    "BATCH_SIZE         = 64\n",
    "IMAGE_SIZE         = (64, 64)            # Tiny‑ImageNet native size\n",
    "LEARNING_RATE      = 0.002271\n",
    "WEIGHT_DECAY       = 1e-5\n",
    "RUN_DIR            = Path(\"runs/tinyimagenet_cnn_vs_kan\").resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"➡️  Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------  utils  -------------------------------------\n",
    "def _force_rgb(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure 3‑channel RGB.\n",
    "\n",
    "    • If RGBA ➜ drop alpha.\n",
    "    • If grayscale ➜ replicate to 3 channels.\n",
    "    \"\"\"\n",
    "    if img.shape[0] == 4:        # RGBA → RGB\n",
    "        return img[:3]\n",
    "    if img.shape[0] == 1:        # Gray → RGB by tiling\n",
    "        return img.repeat(3, 1, 1)\n",
    "    return img                   # already RGB\n",
    "\n",
    "def get_dataloaders(batch_size:int=BATCH_SIZE,\n",
    "                    image_size:Tuple[int,int]=IMAGE_SIZE,\n",
    "                    dataset_name:str=\"zh-plus/tiny-imagenet\"):\n",
    "    \"\"\"\n",
    "    Return train/val/test DataLoaders for Tiny‑ImageNet.\n",
    "\n",
    "    Splits:\n",
    "        • train  – 100 000 imgs\n",
    "        • valid  – 10 000 imgs (used here as held‑out **test**)\n",
    "    We further split 10 % of *train* into an internal validation set.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    # carve INTERNAL val from train\n",
    "    split = ds[\"train\"].train_test_split(\n",
    "        test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n",
    "    train_ds = split[\"train\"]\n",
    "    val_ds   = split[\"test\"]\n",
    "    test_ds  = ds[\"valid\"]                 # official validation → test\n",
    "\n",
    "    # ImageNet normalisation stats\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    tfms = Compose([\n",
    "        ToImage(),                       # PIL → (C,H,W) uint8 tensor\n",
    "        Lambda(_force_rgb),              # ⬅ squash grayscale → RGB\n",
    "        RandomHorizontalFlip(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    def add_tfms(example):\n",
    "        example[\"image\"] = tfms(example[\"image\"])\n",
    "        return example\n",
    "\n",
    "    for split_ds in (train_ds, val_ds, test_ds):\n",
    "        split_ds.set_transform(add_tfms)\n",
    "\n",
    "    loader_cfg = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "    val_loader   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "    test_loader  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  models  --------------------------------------------\n",
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"A lightweight CNN with two conv blocks + MLP head.\"\"\"\n",
    "    def __init__(self, input_shape=(3, *IMAGE_SIZE),   # ⬅ 3‑channel\n",
    "                 conv_channels=(64, 128, 256)):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2, c3 = conv_channels\n",
    "        dropout = 0.1\n",
    "        self.features = nn.Sequential(\n",
    "        # Block 1\n",
    "        nn.Conv2d(C_in, c1, 3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.SiLU(inplace=True),\n",
    "        \n",
    "        # Block 2\n",
    "        nn.Conv2d(c1, c2, 3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.MaxPool2d(2),  # Now 32x32\n",
    "        \n",
    "        # Block 3\n",
    "        nn.Conv2d(c2, c3, 3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            flat_feats = self.features(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "                \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(flat_feats, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 200)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. deps ──────────────────────────────────────────────────────────────\n",
    "# pip install bayesian-optimization if you haven’t already\n",
    "from bayes_opt import BayesianOptimization\n",
    "import torch, time, json\n",
    "from pathlib import Path\n",
    "from classes.BSplineActivation import BSplineActivation\n",
    "# ── 1. make KANCNN fully hyper‑param friendly ────────────────────────────\n",
    "class KANCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    7×7 stem + four two‑conv blocks (64‑128‑256‑512) → GAP → 3‑layer KAN head.\n",
    "    Every width / spline knob is exposed for BayesianOptimization.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(3, *IMAGE_SIZE),\n",
    "                 stem_out=64,          # first 7×7 conv channels\n",
    "                 ch2=128, ch3=256, ch4=512,   # block widths\n",
    "                 kan_1=512, kan_2=256, kan_3=200,\n",
    "                 spline_cp=8, spline_deg=3,\n",
    "                 range_min=-5.0, range_max=5.0):\n",
    "        super().__init__()\n",
    "        C_in, *_ = input_shape\n",
    "\n",
    "        # ── 1. Convolutional backbone ──────────────────────────────────\n",
    "        self.features = nn.Sequential(\n",
    "            # Stem: 64×64 → 32×32\n",
    "            nn.Conv2d(C_in, stem_out, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(stem_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 32×32 → 16×16\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Block 1 (64 channels, no down‑sample) 16×16 → 16×16\n",
    "            nn.Conv2d(stem_out, stem_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(stem_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(stem_out, stem_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(stem_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Block 2 (128 channels, stride‑2) 16×16 → 8×8\n",
    "            nn.Conv2d(stem_out, ch2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch2, ch2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Block 3 (256 channels, stride‑2) 8×8 → 4×4\n",
    "            nn.Conv2d(ch2, ch3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch3, ch3, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Block 4 (512 channels, stride‑2) 4×4 → 2×2\n",
    "            nn.Conv2d(ch3, ch4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch4, ch4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Global average pool 2×2 → 1×1\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        flat_dim = ch4  # after GAP you always have 512 (or ch4)\n",
    "\n",
    "        # ── 2. KAN head with B‑spline activations ───────────────────────\n",
    "        self.kan1 = nn.Linear(flat_dim, kan_1)\n",
    "        self.kan1_act = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan2 = nn.Linear(kan_1, kan_2)\n",
    "        self.kan2_act = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan3 = nn.Linear(kan_2, kan_3)\n",
    "        self.kan3_act = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "\n",
    "    # ── 3. Forward pass ────────────────────────────────────────────────\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)          # B × 512 × 1 × 1\n",
    "        x = torch.flatten(x, 1)       # B × 512\n",
    "        x = self.kan1_act(self.kan1(x))\n",
    "        x = self.kan2_act(self.kan2(x))\n",
    "        x = self.kan3_act(self.kan3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  training / eval helpers  ---------------------------\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Train {epoch:02d}\", leave=False):\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item()  * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item() * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "def run_training(model:nn.Module, name:str,\n",
    "                 train_loader:DataLoader, val_loader:DataLoader) -> Dict[str, List[float]]:\n",
    "    \"\"\"Full training loop for one model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        tic = time.time()\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "        val_metrics   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch:2d}/{N_EPOCHS} • \"\n",
    "              f\"train acc {train_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"val acc {val_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"Δt {time.time()-tic:4.1f}s\")\n",
    "\n",
    "    torch.save(model.state_dict(), RUN_DIR/f\"{name}.pt\")\n",
    "    with open(RUN_DIR/f\"{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df:pd.DataFrame):\n",
    "    \"\"\"Plot accuracy + loss curves for both models.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_acc\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_acc\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_acc\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_acc\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Accuracy vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"accuracy_curves.png\", dpi=200)\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_loss\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_loss\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_loss\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_loss\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Cross‑entropy loss\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Loss vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"loss_curves.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Final test evaluation\n",
    "    \n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "baseline = BaselineCNN()\n",
    "kan      = KANCNN()\n",
    "\n",
    "print(f'KAN parameters: {count_parameters(kan)}')\n",
    "print(f'Baseline parameters: {count_parameters(baseline)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"📚 Training baseline CNN …\")\n",
    "#hist_base, baseline_model = run_training(baseline, \"baseline\", train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🌀 Training KAN‑CNN …\")\n",
    "hist_kan  = run_training(kan,      \"KAN\",      train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#test_base = evaluate(baseline, test_loader, criterion)\n",
    "test_kan  = evaluate(kan,      test_loader, criterion)\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Test accuracy: \"\n",
    "        #f\"Baseline {test_base['acc']*100:5.2f}% | \"\n",
    "        f\"KAN {test_kan['acc']*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    # ints → we pass floats in but will round later\n",
    "    \"epochs\":             (25, 55),\n",
    "    \"kan_1\":          (200, 400),   # width of first KAN layer\n",
    "    \"kan_2\":          (128, 512),   # second KAN layer\n",
    "    \"kan_3\":            (200, 200),\n",
    "    \"spline_cp\":          (6, 10),      # control points\n",
    "    \"spline_deg\":         (2, 5),      # deg ≤ cp‑1 guard enforced later\n",
    "    \"range_min\":          (-5.0, -0.5),\n",
    "    \"range_max\":          (30.0, 70.0),\n",
    "    \"lr\":                 (1e-4, 1e-2)\n",
    "}\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_kan(model, train_loader, val_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tic = time.time()\n",
    "        # ─ train ─\n",
    "        model.train()\n",
    "        loss_sum = acc_sum = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            acc = (logits.argmax(1) == y).float().mean().item()\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            acc_sum  += acc * x.size(0)\n",
    "\n",
    "        # ─ eval ─\n",
    "        model.eval()\n",
    "        loss_sum_val = acc_sum_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                logits = model(x)\n",
    "                loss   = criterion(logits, y)\n",
    "                acc    = (logits.argmax(1) == y).float().mean().item()\n",
    "                loss_sum_val += loss.item() * x.size(0)\n",
    "                acc_sum_val  += acc * x.size(0)\n",
    "\n",
    "        train_loss = loss_sum     / len(train_loader.dataset)\n",
    "        train_acc  = acc_sum      / len(train_loader.dataset)\n",
    "        val_loss   = loss_sum_val / len(val_loader.dataset)\n",
    "        val_acc    = acc_sum_val  / len(val_loader.dataset)\n",
    "        elapsed    = time.time() - tic\n",
    "\n",
    "        # ← print exactly like you had it\n",
    "        print(f\"Epoch [{ep}/{epochs}], \"\n",
    "              f\"Loss: {train_loss:.4f}, \"\n",
    "              f\"Test Acc: {val_acc*100:5.2f}%, \"\n",
    "              f\"Time: {elapsed:5.2f} seconds\")\n",
    "\n",
    "        best_val = max(best_val, val_acc)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "\n",
    "def optimize_kan(epochs,\n",
    "                 kan_1,\n",
    "                 kan_2,\n",
    "                 kan_3,\n",
    "                 spline_cp,\n",
    "                 spline_deg,\n",
    "                 range_min,\n",
    "                 range_max,\n",
    "                 lr):\n",
    "\n",
    "    # ─ cast + sanity ─\n",
    "    epochs      = int(round(epochs))\n",
    "    kan_1   = int(round(kan_1))\n",
    "    kan_2   = int(round(kan_2))\n",
    "    kan_3   = int(round(kan_3))\n",
    "    spline_cp   = int(round(spline_cp))\n",
    "    spline_deg  = int(round(spline_deg))\n",
    "\n",
    "    # keep B‑spline well‑formed\n",
    "    spline_deg  = max(2, min(spline_deg, spline_cp - 1))\n",
    "    lr          = float(lr)\n",
    "\n",
    "    model = KANCNN(\n",
    "        kan_1=kan_1,\n",
    "        kan_2=kan_2,\n",
    "        kan_3=kan_3,\n",
    "        spline_cp=spline_cp,\n",
    "        spline_deg=spline_deg,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max\n",
    "    )\n",
    "\n",
    "    val_acc = train_kan(model, train_loader, val_loader, epochs, lr)\n",
    "\n",
    "    # BayesOpt maximizes the returned value\n",
    "    return val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_kan,\n",
    "    pbounds=pbounds,\n",
    "    random_state=38,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=6, n_iter=10)\n",
    "\n",
    "print(\"🚀 best combo so far →\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
