{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, Union, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip, ToImage,\n",
    "    ToDtype, Normalize, Lambda                    # â¬… new\n",
    ")\n",
    "from torchvision.utils import make_grid\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸  Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# -----------------------  constants  ---------------------------------\n",
    "SEED               = 42\n",
    "N_EPOCHS           = 10\n",
    "BATCH_SIZE         = 128\n",
    "IMAGE_SIZE         = (64, 64)            # Tinyâ€‘ImageNet native size\n",
    "LEARNING_RATE      = 3e-4\n",
    "WEIGHT_DECAY       = 1e-5\n",
    "RUN_DIR            = Path(\"runs/tinyimagenet_cnn_vs_kan\").resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"âž¡ï¸  Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------  utils  -------------------------------------\n",
    "def _force_rgb(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure 3â€‘channel RGB.\n",
    "\n",
    "    â€¢ If RGBA âžœ drop alpha.\n",
    "    â€¢ If grayscale âžœ replicate to 3 channels.\n",
    "    \"\"\"\n",
    "    if img.shape[0] == 4:        # RGBA â†’ RGB\n",
    "        return img[:3]\n",
    "    if img.shape[0] == 1:        # Gray â†’ RGB by tiling\n",
    "        return img.repeat(3, 1, 1)\n",
    "    return img                   # already RGB\n",
    "\n",
    "def get_dataloaders(batch_size:int=BATCH_SIZE,\n",
    "                    image_size:Tuple[int,int]=IMAGE_SIZE,\n",
    "                    dataset_name:str=\"zh-plus/tiny-imagenet\"):\n",
    "    \"\"\"\n",
    "    Return train/val/test DataLoaders for Tinyâ€‘ImageNet.\n",
    "\n",
    "    Splits:\n",
    "        â€¢ train  â€“ 100â€¯000 imgs\n",
    "        â€¢ valid  â€“ 10â€¯000 imgs (used here as heldâ€‘out **test**)\n",
    "    We further split 10â€¯% of *train* into an internal validation set.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    # carve INTERNAL val from train\n",
    "    split = ds[\"train\"].train_test_split(\n",
    "        test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n",
    "    train_ds = split[\"train\"]\n",
    "    val_ds   = split[\"test\"]\n",
    "    test_ds  = ds[\"valid\"]                 # official validation â†’ test\n",
    "\n",
    "    # ImageNet normalisation stats\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    tfms = Compose([\n",
    "        ToImage(),                       # PIL â†’ (C,H,W) uint8 tensor\n",
    "        Lambda(_force_rgb),              # â¬… squash grayscale â†’ RGB\n",
    "        RandomHorizontalFlip(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    def add_tfms(example):\n",
    "        example[\"image\"] = tfms(example[\"image\"])\n",
    "        return example\n",
    "\n",
    "    for split_ds in (train_ds, val_ds, test_ds):\n",
    "        split_ds.set_transform(add_tfms)\n",
    "\n",
    "    loader_cfg = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "    val_loader   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "    test_loader  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  models  --------------------------------------------\n",
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"A lightweight CNN with two conv blocks + MLP head.\"\"\"\n",
    "    def __init__(self, input_shape=(3, *IMAGE_SIZE),   # â¬… 3â€‘channel\n",
    "                 conv_channels=(64, 128, 256)):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2, c3 = conv_channels\n",
    "        dropout = 0.1\n",
    "        self.features = nn.Sequential(\n",
    "        # Block 1\n",
    "        nn.Conv2d(C_in, c1, 3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.SiLU(inplace=True),\n",
    "        \n",
    "        # Block 2\n",
    "        nn.Conv2d(c1, c2, 3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.MaxPool2d(2),  # Now 32x32\n",
    "        \n",
    "        # Block 3\n",
    "        nn.Conv2d(c2, c3, 3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            flat_feats = self.features(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "                \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(flat_feats, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 200)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 0. deps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# pip install bayesian-optimization if you havenâ€™t already\n",
    "from bayes_opt import BayesianOptimization\n",
    "import torch, time, json\n",
    "from pathlib import Path\n",
    "from classes.BSplineActivation import BSplineActivation\n",
    "# â”€â”€ 1. make KANCNN fully hyperâ€‘param friendly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class KANCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Same backbone as before but every spline & layer size is configurable,\n",
    "    so BayesOpt can mess with them.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(3, *IMAGE_SIZE),\n",
    "                 conv_channels=(64, 128),\n",
    "                 kan_1=512,\n",
    "                 kan_2=256,\n",
    "                 kan_3=200,\n",
    "                 spline_cp=7,\n",
    "                 spline_deg=2,\n",
    "                 range_min=-3.0,\n",
    "                 range_max=50.0):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2 = conv_channels\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(C_in, c1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c2), nn.ReLU(inplace=True), nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flat = self.features(torch.zeros(1, *input_shape)).flatten(1).size(1)\n",
    "\n",
    "        self.kan1      = nn.Linear(flat, kan_1)\n",
    "        self.kan1_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan2      = nn.Linear(kan_1, kan_2)\n",
    "        self.kan2_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan3      = nn.Linear(kan_2, kan_3)\n",
    "        self.kan3_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.features(x), 1)\n",
    "        x = self.kan1(x)\n",
    "        x = self.kan1_act(x)\n",
    "        x = self.kan2(x)\n",
    "        x = self.kan2_act(x)\n",
    "        x = self.kan3(x)\n",
    "        x = self.kan3_act(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  training / eval helpers  ---------------------------\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Train {epoch:02d}\", leave=False):\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item()  * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item() * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "def run_training(model:nn.Module, name:str,\n",
    "                 train_loader:DataLoader, val_loader:DataLoader) -> Dict[str, List[float]]:\n",
    "    \"\"\"Full training loop for one model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        tic = time.time()\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "        val_metrics   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch:2d}/{N_EPOCHS} â€¢ \"\n",
    "              f\"train acc {train_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"val acc {val_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"Î”t {time.time()-tic:4.1f}s\")\n",
    "\n",
    "    torch.save(model.state_dict(), RUN_DIR/f\"{name}.pt\")\n",
    "    with open(RUN_DIR/f\"{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df:pd.DataFrame):\n",
    "    \"\"\"Plot accuracy + loss curves for both models.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_acc\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_acc\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_acc\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_acc\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Tinyâ€‘ImageNet â€¢ Accuracy vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"accuracy_curves.png\", dpi=200)\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_loss\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_loss\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_loss\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_loss\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Crossâ€‘entropy loss\")\n",
    "    ax.set_title(\"Tinyâ€‘ImageNet â€¢ Loss vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"loss_curves.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAN parameters: 17036509\n",
      "Baseline parameters: 13661840\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Final test evaluation\n",
    "    \n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "baseline = BaselineCNN()\n",
    "kan      = KANCNN()\n",
    "\n",
    "print(f'KAN parameters: {count_parameters(kan)}')\n",
    "print(f'Baseline parameters: {count_parameters(baseline)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"ðŸ“š Training baseline CNN â€¦\")\n",
    "#hist_base, baseline_model = run_training(baseline, \"baseline\", train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nðŸŒ€ Training KANâ€‘CNN â€¦\")\n",
    "#hist_kan  = run_training(kan,      \"KAN\",      train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncriterion = nn.CrossEntropyLoss()\\n#test_base = evaluate(baseline, test_loader, criterion)\\ntest_kan  = evaluate(kan,      test_loader, criterion)\\n\\n\\nprint(f\"\\nâœ… Test accuracy: \"\\n        #f\"Baseline {test_base[\\'acc\\']*100:5.2f}% | \"\\n        f\"KAN {test_kan[\\'acc\\']*100:5.2f}%\")\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#test_base = evaluate(baseline, test_loader, criterion)\n",
    "test_kan  = evaluate(kan,      test_loader, criterion)\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Test accuracy: \"\n",
    "        #f\"Baseline {test_base['acc']*100:5.2f}% | \"\n",
    "        f\"KAN {test_kan['acc']*100:5.2f}%\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    # ints â†’ we pass floats in but will round later\n",
    "    \"epochs\":             (35, 55),\n",
    "    \"kan_1\":          (64, 512),   # width of first KAN layer\n",
    "    \"kan_2\":          (128, 512),   # second KAN layer\n",
    "    \"kan_3\":            (200, 200),\n",
    "    \"spline_cp\":          (6, 10),      # control points\n",
    "    \"spline_deg\":         (2, 5),      # deg â‰¤ cpâ€‘1 guard enforced later\n",
    "    \"range_min\":          (-5.0, -0.5),\n",
    "    \"range_max\":          (5.0, 70.0),\n",
    "    \"lr\":                 (1e-4, 5e-3)\n",
    "}\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_kan(model, train_loader, val_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tic = time.time()\n",
    "        # â”€ train â”€\n",
    "        model.train()\n",
    "        loss_sum = acc_sum = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            acc = (logits.argmax(1) == y).float().mean().item()\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            acc_sum  += acc * x.size(0)\n",
    "\n",
    "        # â”€ eval â”€\n",
    "        model.eval()\n",
    "        loss_sum_val = acc_sum_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                logits = model(x)\n",
    "                loss   = criterion(logits, y)\n",
    "                acc    = (logits.argmax(1) == y).float().mean().item()\n",
    "                loss_sum_val += loss.item() * x.size(0)\n",
    "                acc_sum_val  += acc * x.size(0)\n",
    "\n",
    "        train_loss = loss_sum     / len(train_loader.dataset)\n",
    "        train_acc  = acc_sum      / len(train_loader.dataset)\n",
    "        val_loss   = loss_sum_val / len(val_loader.dataset)\n",
    "        val_acc    = acc_sum_val  / len(val_loader.dataset)\n",
    "        elapsed    = time.time() - tic\n",
    "\n",
    "        # â† print exactly like you had it\n",
    "        print(f\"Epoch [{ep}/{epochs}], \"\n",
    "              f\"Loss: {train_loss:.4f}, \"\n",
    "              f\"Test Acc: {val_acc*100:5.2f}%, \"\n",
    "              f\"Time: {elapsed:5.2f} seconds\")\n",
    "\n",
    "        best_val = max(best_val, val_acc)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "\n",
    "def optimize_kan(epochs,\n",
    "                 kan_inner,\n",
    "                 kan_outer,\n",
    "                 spline_cp,\n",
    "                 spline_deg,\n",
    "                 range_min,\n",
    "                 range_max,\n",
    "                 lr):\n",
    "\n",
    "    # â”€ cast + sanity â”€\n",
    "    epochs      = int(round(epochs))\n",
    "    kan_inner   = int(round(kan_inner))\n",
    "    kan_outer   = int(round(kan_outer))\n",
    "    spline_cp   = int(round(spline_cp))\n",
    "    spline_deg  = int(round(spline_deg))\n",
    "\n",
    "    # keep Bâ€‘spline wellâ€‘formed\n",
    "    spline_deg  = max(2, min(spline_deg, spline_cp - 1))\n",
    "    lr          = float(lr)\n",
    "\n",
    "    model = KANCNN(\n",
    "        kan_inner=kan_inner,\n",
    "        kan_outer=kan_outer,\n",
    "        spline_cp=spline_cp,\n",
    "        spline_deg=spline_deg,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max\n",
    "    )\n",
    "\n",
    "    val_acc = train_kan(model, train_loader, val_loader, epochs, lr)\n",
    "\n",
    "    # BayesOpt maximizes the returned value\n",
    "    return val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  epochs   | kan_inner | kan_outer |    lr     | range_max | range_min | spline_cp | spline... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1/31], Loss: 5.0894, Test Acc:  2.46%, Time: 39.94 seconds\n",
      "Epoch [2/31], Loss: 4.6430, Test Acc:  5.15%, Time: 40.37 seconds\n",
      "Epoch [3/31], Loss: 4.3431, Test Acc:  3.28%, Time: 40.49 seconds\n",
      "Epoch [4/31], Loss: 4.1074, Test Acc:  6.10%, Time: 40.69 seconds\n",
      "Epoch [5/31], Loss: 3.9047, Test Acc:  8.55%, Time: 40.71 seconds\n",
      "Epoch [6/31], Loss: 3.7491, Test Acc:  5.76%, Time: 40.74 seconds\n",
      "Epoch [7/31], Loss: 3.5998, Test Acc:  3.77%, Time: 40.68 seconds\n",
      "Epoch [8/31], Loss: 3.4954, Test Acc:  8.44%, Time: 40.71 seconds\n",
      "Epoch [9/31], Loss: 3.3709, Test Acc: 17.48%, Time: 40.87 seconds\n",
      "Epoch [10/31], Loss: 3.2917, Test Acc: 11.84%, Time: 40.85 seconds\n",
      "Epoch [11/31], Loss: 3.1975, Test Acc:  4.56%, Time: 40.85 seconds\n",
      "Epoch [12/31], Loss: 3.0924, Test Acc:  8.87%, Time: 40.85 seconds\n",
      "Epoch [13/31], Loss: 3.0132, Test Acc: 10.50%, Time: 1344.86 seconds\n",
      "Epoch [14/31], Loss: 2.9334, Test Acc:  9.71%, Time: 114.55 seconds\n",
      "Epoch [15/31], Loss: 2.8427, Test Acc: 11.44%, Time: 40.71 seconds\n",
      "Epoch [16/31], Loss: 2.7779, Test Acc:  7.09%, Time: 40.80 seconds\n",
      "Epoch [17/31], Loss: 2.6878, Test Acc:  7.08%, Time: 40.93 seconds\n",
      "Epoch [18/31], Loss: 2.5988, Test Acc:  7.71%, Time: 40.94 seconds\n",
      "Epoch [19/31], Loss: 2.5411, Test Acc: 14.87%, Time: 41.02 seconds\n",
      "Epoch [20/31], Loss: 2.4552, Test Acc: 13.08%, Time: 41.03 seconds\n",
      "Epoch [21/31], Loss: 2.4017, Test Acc: 13.50%, Time: 40.84 seconds\n",
      "Epoch [22/31], Loss: 2.3381, Test Acc:  9.01%, Time: 41.01 seconds\n",
      "Epoch [23/31], Loss: 2.2643, Test Acc: 19.35%, Time: 41.04 seconds\n",
      "Epoch [24/31], Loss: 2.1856, Test Acc:  8.53%, Time: 40.91 seconds\n",
      "Epoch [25/31], Loss: 2.1077, Test Acc:  9.40%, Time: 40.91 seconds\n",
      "Epoch [26/31], Loss: 2.0385, Test Acc: 13.44%, Time: 40.98 seconds\n",
      "Epoch [27/31], Loss: 1.9813, Test Acc:  7.19%, Time: 40.99 seconds\n",
      "Epoch [28/31], Loss: 1.8961, Test Acc: 13.51%, Time: 40.92 seconds\n",
      "Epoch [29/31], Loss: 1.8443, Test Acc:  9.35%, Time: 40.98 seconds\n",
      "Epoch [30/31], Loss: 1.7638, Test Acc:  8.92%, Time: 40.93 seconds\n",
      "Epoch [31/31], Loss: 1.7158, Test Acc:  8.06%, Time: 41.01 seconds\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.1935   \u001b[39m | \u001b[39m30.77    \u001b[39m | \u001b[39m449.1    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.0007325\u001b[39m | \u001b[39m41.68    \u001b[39m | \u001b[39m-2.576   \u001b[39m | \u001b[39m7.001    \u001b[39m | \u001b[39m3.162    \u001b[39m |\n",
      "Epoch [1/29], Loss: 5.2026, Test Acc:  1.37%, Time: 37.13 seconds\n",
      "Epoch [2/29], Loss: 4.8844, Test Acc:  1.89%, Time: 36.90 seconds\n",
      "Epoch [3/29], Loss: 4.6158, Test Acc:  3.81%, Time: 37.05 seconds\n",
      "Epoch [4/29], Loss: 4.3760, Test Acc:  5.51%, Time: 36.96 seconds\n",
      "Epoch [5/29], Loss: 4.2172, Test Acc:  3.46%, Time: 36.95 seconds\n",
      "Epoch [6/29], Loss: 4.0927, Test Acc:  5.18%, Time: 37.01 seconds\n",
      "Epoch [7/29], Loss: 3.9657, Test Acc: 12.54%, Time: 37.12 seconds\n",
      "Epoch [8/29], Loss: 3.8756, Test Acc: 10.83%, Time: 37.19 seconds\n",
      "Epoch [9/29], Loss: 3.7932, Test Acc: 10.53%, Time: 36.96 seconds\n",
      "Epoch [10/29], Loss: 3.7138, Test Acc:  9.18%, Time: 37.13 seconds\n",
      "Epoch [11/29], Loss: 3.6554, Test Acc: 17.86%, Time: 36.99 seconds\n",
      "Epoch [12/29], Loss: 3.6034, Test Acc:  6.86%, Time: 36.68 seconds\n",
      "Epoch [13/29], Loss: 3.5637, Test Acc:  9.32%, Time: 37.15 seconds\n",
      "Epoch [14/29], Loss: 3.4957, Test Acc:  9.66%, Time: 36.53 seconds\n",
      "Epoch [15/29], Loss: 3.4432, Test Acc:  4.30%, Time: 36.73 seconds\n",
      "Epoch [16/29], Loss: 3.3961, Test Acc: 17.03%, Time: 36.62 seconds\n",
      "Epoch [17/29], Loss: 3.3552, Test Acc: 20.40%, Time: 36.94 seconds\n",
      "Epoch [18/29], Loss: 3.3284, Test Acc:  7.51%, Time: 37.10 seconds\n",
      "Epoch [19/29], Loss: 3.2927, Test Acc: 10.16%, Time: 36.48 seconds\n",
      "Epoch [20/29], Loss: 3.2467, Test Acc:  6.69%, Time: 36.38 seconds\n",
      "Epoch [21/29], Loss: 3.2083, Test Acc:  9.73%, Time: 36.75 seconds\n",
      "Epoch [22/29], Loss: 3.1765, Test Acc:  8.50%, Time: 36.92 seconds\n",
      "Epoch [23/29], Loss: 3.1816, Test Acc: 10.45%, Time: 37.15 seconds\n",
      "Epoch [24/29], Loss: 3.1174, Test Acc:  5.26%, Time: 36.76 seconds\n",
      "Epoch [25/29], Loss: 3.1055, Test Acc: 21.88%, Time: 36.51 seconds\n",
      "Epoch [26/29], Loss: 3.0604, Test Acc: 10.70%, Time: 36.96 seconds\n",
      "Epoch [27/29], Loss: 3.0293, Test Acc:  4.11%, Time: 37.58 seconds\n",
      "Epoch [28/29], Loss: 3.0098, Test Acc: 15.94%, Time: 36.90 seconds\n",
      "Epoch [29/29], Loss: 2.9840, Test Acc:  6.44%, Time: 36.97 seconds\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.2188   \u001b[39m | \u001b[35m28.88    \u001b[39m | \u001b[35m97.42    \u001b[39m | \u001b[35m200.0    \u001b[39m | \u001b[35m0.0004946\u001b[39m | \u001b[35m34.16    \u001b[39m | \u001b[35m-1.526   \u001b[39m | \u001b[35m8.648    \u001b[39m | \u001b[35m3.687    \u001b[39m |\n",
      "Epoch [1/32], Loss: 5.1113, Test Acc:  5.46%, Time: 40.38 seconds\n",
      "Epoch [2/32], Loss: 4.6852, Test Acc:  4.07%, Time: 40.04 seconds\n",
      "Epoch [3/32], Loss: 4.4111, Test Acc:  3.88%, Time: 40.07 seconds\n",
      "Epoch [4/32], Loss: 4.1912, Test Acc:  4.11%, Time: 40.47 seconds\n",
      "Epoch [5/32], Loss: 4.0133, Test Acc:  4.11%, Time: 39.96 seconds\n",
      "Epoch [6/32], Loss: 3.8784, Test Acc:  5.12%, Time: 41.18 seconds\n",
      "Epoch [7/32], Loss: 3.7560, Test Acc: 17.42%, Time: 40.60 seconds\n",
      "Epoch [8/32], Loss: 3.6484, Test Acc:  3.32%, Time: 40.45 seconds\n",
      "Epoch [9/32], Loss: 3.5723, Test Acc: 15.18%, Time: 39.91 seconds\n",
      "Epoch [10/32], Loss: 3.4649, Test Acc:  8.49%, Time: 40.96 seconds\n",
      "Epoch [11/32], Loss: 3.4121, Test Acc:  4.81%, Time: 39.90 seconds\n",
      "Epoch [12/32], Loss: 3.3397, Test Acc:  6.08%, Time: 39.52 seconds\n",
      "Epoch [13/32], Loss: 3.2535, Test Acc:  9.18%, Time: 39.66 seconds\n",
      "Epoch [14/32], Loss: 3.2033, Test Acc:  8.23%, Time: 39.95 seconds\n",
      "Epoch [15/32], Loss: 3.1370, Test Acc: 10.20%, Time: 40.05 seconds\n",
      "Epoch [16/32], Loss: 3.0600, Test Acc:  9.83%, Time: 40.82 seconds\n",
      "Epoch [17/32], Loss: 3.0035, Test Acc: 10.64%, Time: 40.18 seconds\n",
      "Epoch [18/32], Loss: 2.9547, Test Acc: 18.97%, Time: 40.05 seconds\n",
      "Epoch [19/32], Loss: 2.9055, Test Acc:  4.57%, Time: 40.32 seconds\n",
      "Epoch [20/32], Loss: 2.8463, Test Acc: 11.05%, Time: 40.00 seconds\n",
      "Epoch [21/32], Loss: 2.7958, Test Acc: 12.40%, Time: 40.09 seconds\n",
      "Epoch [22/32], Loss: 2.7216, Test Acc: 11.06%, Time: 39.91 seconds\n",
      "Epoch [23/32], Loss: 2.6759, Test Acc: 12.10%, Time: 39.95 seconds\n",
      "Epoch [24/32], Loss: 2.6313, Test Acc: 10.27%, Time: 39.99 seconds\n",
      "Epoch [25/32], Loss: 2.5644, Test Acc:  9.85%, Time: 39.99 seconds\n",
      "Epoch [26/32], Loss: 2.4940, Test Acc: 12.81%, Time: 40.05 seconds\n",
      "Epoch [27/32], Loss: 2.4375, Test Acc:  8.85%, Time: 39.96 seconds\n",
      "Epoch [28/32], Loss: 2.3939, Test Acc: 10.52%, Time: 40.20 seconds\n",
      "Epoch [29/32], Loss: 2.3439, Test Acc: 25.25%, Time: 39.88 seconds\n",
      "Epoch [30/32], Loss: 2.2972, Test Acc: 10.54%, Time: 39.90 seconds\n",
      "Epoch [31/32], Loss: 2.2386, Test Acc:  9.29%, Time: 39.92 seconds\n",
      "Epoch [32/32], Loss: 2.1594, Test Acc: 10.19%, Time: 39.90 seconds\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.2525   \u001b[39m | \u001b[35m32.07    \u001b[39m | \u001b[35m372.6    \u001b[39m | \u001b[35m200.0    \u001b[39m | \u001b[35m0.0008409\u001b[39m | \u001b[35m58.02    \u001b[39m | \u001b[35m-1.567   \u001b[39m | \u001b[35m7.546    \u001b[39m | \u001b[35m3.649    \u001b[39m |\n",
      "Epoch [1/35], Loss: 5.2012, Test Acc:  1.48%, Time: 41.63 seconds\n",
      "Epoch [2/35], Loss: 4.9006, Test Acc:  1.62%, Time: 40.97 seconds\n",
      "Epoch [3/35], Loss: 4.6420, Test Acc:  3.10%, Time: 40.84 seconds\n",
      "Epoch [4/35], Loss: 4.4266, Test Acc:  2.41%, Time: 41.11 seconds\n",
      "Epoch [5/35], Loss: 4.2414, Test Acc:  2.94%, Time: 41.06 seconds\n",
      "Epoch [6/35], Loss: 4.0844, Test Acc: 14.88%, Time: 41.35 seconds\n",
      "Epoch [7/35], Loss: 3.9367, Test Acc:  7.67%, Time: 41.13 seconds\n",
      "Epoch [8/35], Loss: 3.8073, Test Acc:  7.78%, Time: 41.95 seconds\n",
      "Epoch [9/35], Loss: 3.6993, Test Acc:  6.19%, Time: 41.41 seconds\n",
      "Epoch [10/35], Loss: 3.5842, Test Acc:  8.40%, Time: 41.51 seconds\n",
      "Epoch [11/35], Loss: 3.4876, Test Acc:  7.19%, Time: 41.38 seconds\n",
      "Epoch [12/35], Loss: 3.4004, Test Acc: 14.41%, Time: 41.42 seconds\n",
      "Epoch [13/35], Loss: 3.3146, Test Acc: 20.34%, Time: 42.05 seconds\n",
      "Epoch [14/35], Loss: 3.2513, Test Acc:  7.15%, Time: 41.44 seconds\n",
      "Epoch [15/35], Loss: 3.1565, Test Acc: 10.81%, Time: 41.45 seconds\n",
      "Epoch [16/35], Loss: 3.0759, Test Acc:  9.67%, Time: 41.38 seconds\n",
      "Epoch [17/35], Loss: 3.0209, Test Acc:  7.07%, Time: 41.53 seconds\n",
      "Epoch [18/35], Loss: 2.9460, Test Acc: 12.84%, Time: 41.66 seconds\n",
      "Epoch [19/35], Loss: 2.8764, Test Acc: 10.92%, Time: 41.05 seconds\n",
      "Epoch [20/35], Loss: 2.8110, Test Acc: 12.87%, Time: 41.60 seconds\n",
      "Epoch [21/35], Loss: 2.7272, Test Acc: 22.43%, Time: 41.21 seconds\n",
      "Epoch [22/35], Loss: 2.6688, Test Acc:  4.54%, Time: 41.29 seconds\n",
      "Epoch [23/35], Loss: 2.6004, Test Acc:  5.09%, Time: 41.44 seconds\n",
      "Epoch [24/35], Loss: 2.5322, Test Acc: 19.09%, Time: 41.40 seconds\n",
      "Epoch [25/35], Loss: 2.4896, Test Acc:  5.78%, Time: 40.96 seconds\n",
      "Epoch [26/35], Loss: 2.4037, Test Acc:  8.86%, Time: 41.18 seconds\n",
      "Epoch [27/35], Loss: 2.3322, Test Acc: 10.17%, Time: 41.66 seconds\n",
      "Epoch [28/35], Loss: 2.2722, Test Acc: 23.38%, Time: 41.40 seconds\n",
      "Epoch [29/35], Loss: 2.1945, Test Acc: 12.59%, Time: 41.24 seconds\n",
      "Epoch [30/35], Loss: 2.1388, Test Acc: 11.82%, Time: 41.40 seconds\n",
      "Epoch [31/35], Loss: 2.0769, Test Acc:  5.61%, Time: 41.32 seconds\n",
      "Epoch [32/35], Loss: 2.0259, Test Acc:  5.96%, Time: 41.21 seconds\n",
      "Epoch [33/35], Loss: 1.9858, Test Acc:  9.57%, Time: 41.30 seconds\n",
      "Epoch [34/35], Loss: 1.9097, Test Acc: 23.61%, Time: 41.51 seconds\n",
      "Epoch [35/35], Loss: 1.8412, Test Acc:  4.93%, Time: 41.24 seconds\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.2361   \u001b[39m | \u001b[39m34.82    \u001b[39m | \u001b[39m490.0    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.0005037\u001b[39m | \u001b[39m49.96    \u001b[39m | \u001b[39m-2.087   \u001b[39m | \u001b[39m6.441    \u001b[39m | \u001b[39m4.379    \u001b[39m |\n",
      "Epoch [1/39], Loss: 5.2031, Test Acc:  2.79%, Time: 39.68 seconds\n",
      "Epoch [2/39], Loss: 4.9270, Test Acc:  2.36%, Time: 39.32 seconds\n",
      "Epoch [3/39], Loss: 4.6732, Test Acc:  3.63%, Time: 39.44 seconds\n",
      "Epoch [4/39], Loss: 4.4721, Test Acc:  5.23%, Time: 39.28 seconds\n",
      "Epoch [5/39], Loss: 4.2820, Test Acc:  6.32%, Time: 39.69 seconds\n",
      "Epoch [6/39], Loss: 4.1261, Test Acc:  6.40%, Time: 39.62 seconds\n",
      "Epoch [7/39], Loss: 3.9869, Test Acc: 10.79%, Time: 39.55 seconds\n",
      "Epoch [8/39], Loss: 3.8627, Test Acc:  8.57%, Time: 39.45 seconds\n",
      "Epoch [9/39], Loss: 3.7412, Test Acc: 11.82%, Time: 39.33 seconds\n",
      "Epoch [10/39], Loss: 3.6426, Test Acc: 11.75%, Time: 39.32 seconds\n",
      "Epoch [11/39], Loss: 3.5452, Test Acc:  8.01%, Time: 39.33 seconds\n",
      "Epoch [12/39], Loss: 3.4561, Test Acc: 17.32%, Time: 39.32 seconds\n",
      "Epoch [13/39], Loss: 3.3756, Test Acc: 21.96%, Time: 39.59 seconds\n",
      "Epoch [14/39], Loss: 3.2999, Test Acc: 12.42%, Time: 39.60 seconds\n",
      "Epoch [15/39], Loss: 3.2155, Test Acc: 10.53%, Time: 39.50 seconds\n",
      "Epoch [16/39], Loss: 3.1501, Test Acc: 11.45%, Time: 39.33 seconds\n",
      "Epoch [17/39], Loss: 3.0740, Test Acc: 10.01%, Time: 39.30 seconds\n",
      "Epoch [18/39], Loss: 3.0096, Test Acc:  6.91%, Time: 39.71 seconds\n",
      "Epoch [19/39], Loss: 2.9406, Test Acc:  9.64%, Time: 39.35 seconds\n",
      "Epoch [20/39], Loss: 2.8961, Test Acc:  5.67%, Time: 39.62 seconds\n",
      "Epoch [21/39], Loss: 2.8366, Test Acc: 12.30%, Time: 40.66 seconds\n",
      "Epoch [22/39], Loss: 2.7532, Test Acc:  8.83%, Time: 39.57 seconds\n",
      "Epoch [23/39], Loss: 2.6995, Test Acc: 16.82%, Time: 39.33 seconds\n",
      "Epoch [24/39], Loss: 2.6447, Test Acc: 12.95%, Time: 41.07 seconds\n",
      "Epoch [25/39], Loss: 2.5985, Test Acc: 16.32%, Time: 42.29 seconds\n",
      "Epoch [26/39], Loss: 2.5375, Test Acc:  8.83%, Time: 41.90 seconds\n",
      "Epoch [27/39], Loss: 2.4863, Test Acc: 17.95%, Time: 39.75 seconds\n",
      "Epoch [28/39], Loss: 2.4191, Test Acc: 20.94%, Time: 39.09 seconds\n",
      "Epoch [29/39], Loss: 2.3792, Test Acc: 10.55%, Time: 39.33 seconds\n",
      "Epoch [30/39], Loss: 2.3006, Test Acc: 13.14%, Time: 39.61 seconds\n",
      "Epoch [31/39], Loss: 2.2518, Test Acc: 13.17%, Time: 39.19 seconds\n",
      "Epoch [32/39], Loss: 2.2141, Test Acc: 13.70%, Time: 41.45 seconds\n",
      "Epoch [33/39], Loss: 2.1417, Test Acc: 14.34%, Time: 39.85 seconds\n",
      "Epoch [34/39], Loss: 2.0874, Test Acc: 24.80%, Time: 41.35 seconds\n",
      "Epoch [35/39], Loss: 2.0483, Test Acc:  7.26%, Time: 41.16 seconds\n",
      "Epoch [36/39], Loss: 1.9722, Test Acc:  9.54%, Time: 40.03 seconds\n",
      "Epoch [37/39], Loss: 1.9472, Test Acc:  7.28%, Time: 41.58 seconds\n",
      "Epoch [38/39], Loss: 1.8768, Test Acc:  8.51%, Time: 39.47 seconds\n",
      "Epoch [39/39], Loss: 1.8344, Test Acc:  8.18%, Time: 39.19 seconds\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.248    \u001b[39m | \u001b[39m38.85    \u001b[39m | \u001b[39m339.5    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.0003175\u001b[39m | \u001b[39m30.66    \u001b[39m | \u001b[39m-2.083   \u001b[39m | \u001b[39m6.075    \u001b[39m | \u001b[39m2.553    \u001b[39m |\n",
      "Epoch [1/32], Loss: 5.2343, Test Acc:  1.14%, Time: 39.79 seconds\n",
      "Epoch [2/32], Loss: 5.0102, Test Acc:  0.92%, Time: 39.80 seconds\n",
      "Epoch [3/32], Loss: 4.7874, Test Acc:  1.07%, Time: 40.04 seconds\n",
      "Epoch [4/32], Loss: 4.6151, Test Acc:  1.04%, Time: 39.82 seconds\n",
      "Epoch [5/32], Loss: 4.4762, Test Acc:  3.69%, Time: 39.79 seconds\n",
      "Epoch [6/32], Loss: 4.3631, Test Acc:  6.69%, Time: 39.83 seconds\n",
      "Epoch [7/32], Loss: 4.2470, Test Acc:  2.17%, Time: 40.18 seconds\n",
      "Epoch [8/32], Loss: 4.1368, Test Acc:  7.46%, Time: 1855.85 seconds\n",
      "Epoch [9/32], Loss: 4.0487, Test Acc:  5.22%, Time: 84.28 seconds\n",
      "Epoch [10/32], Loss: 3.9710, Test Acc:  2.97%, Time: 88.70 seconds\n",
      "Epoch [11/32], Loss: 3.8751, Test Acc:  7.26%, Time: 68.56 seconds\n",
      "Epoch [12/32], Loss: 3.8127, Test Acc:  3.76%, Time: 72.91 seconds\n",
      "Epoch [13/32], Loss: 3.7425, Test Acc:  2.54%, Time: 39.13 seconds\n",
      "Epoch [14/32], Loss: 3.6875, Test Acc:  3.14%, Time: 39.64 seconds\n",
      "Epoch [15/32], Loss: 3.6172, Test Acc:  2.99%, Time: 40.31 seconds\n",
      "Epoch [16/32], Loss: 3.5546, Test Acc:  8.01%, Time: 40.28 seconds\n",
      "Epoch [17/32], Loss: 3.5055, Test Acc:  3.65%, Time: 40.71 seconds\n",
      "Epoch [18/32], Loss: 3.4473, Test Acc:  3.61%, Time: 40.36 seconds\n",
      "Epoch [19/32], Loss: 3.4015, Test Acc:  6.11%, Time: 40.51 seconds\n",
      "Epoch [20/32], Loss: 3.3403, Test Acc:  4.78%, Time: 39.91 seconds\n",
      "Epoch [21/32], Loss: 3.2961, Test Acc:  3.22%, Time: 39.83 seconds\n",
      "Epoch [22/32], Loss: 3.2478, Test Acc:  2.92%, Time: 40.00 seconds\n",
      "Epoch [23/32], Loss: 3.1911, Test Acc:  4.94%, Time: 39.83 seconds\n",
      "Epoch [24/32], Loss: 3.1414, Test Acc:  5.48%, Time: 39.90 seconds\n",
      "Epoch [25/32], Loss: 3.1064, Test Acc:  3.29%, Time: 40.13 seconds\n",
      "Epoch [26/32], Loss: 3.0479, Test Acc:  8.96%, Time: 40.63 seconds\n",
      "Epoch [27/32], Loss: 3.0157, Test Acc:  2.56%, Time: 39.83 seconds\n",
      "Epoch [28/32], Loss: 2.9651, Test Acc:  6.50%, Time: 40.48 seconds\n",
      "Epoch [29/32], Loss: 2.9348, Test Acc:  4.15%, Time: 40.13 seconds\n",
      "Epoch [30/32], Loss: 2.8914, Test Acc:  4.44%, Time: 39.51 seconds\n",
      "Epoch [31/32], Loss: 2.8504, Test Acc: 16.48%, Time: 39.61 seconds\n",
      "Epoch [32/32], Loss: 2.8112, Test Acc:  4.29%, Time: 39.46 seconds\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.1648   \u001b[39m | \u001b[39m31.65    \u001b[39m | \u001b[39m332.1    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.0008307\u001b[39m | \u001b[39m20.89    \u001b[39m | \u001b[39m-4.638   \u001b[39m | \u001b[39m6.606    \u001b[39m | \u001b[39m4.976    \u001b[39m |\n",
      "Epoch [1/29], Loss: 5.2607, Test Acc:  2.05%, Time: 41.40 seconds\n",
      "Epoch [2/29], Loss: 5.1331, Test Acc:  1.78%, Time: 40.72 seconds\n",
      "Epoch [3/29], Loss: 4.9899, Test Acc:  2.70%, Time: 41.72 seconds\n",
      "Epoch [4/29], Loss: 4.8479, Test Acc:  4.71%, Time: 41.34 seconds\n",
      "Epoch [5/29], Loss: 4.7101, Test Acc:  4.73%, Time: 40.73 seconds\n",
      "Epoch [6/29], Loss: 4.5826, Test Acc:  4.50%, Time: 41.44 seconds\n",
      "Epoch [7/29], Loss: 4.4679, Test Acc:  3.29%, Time: 41.45 seconds\n",
      "Epoch [8/29], Loss: 4.3581, Test Acc:  8.10%, Time: 40.93 seconds\n",
      "Epoch [9/29], Loss: 4.2549, Test Acc:  7.11%, Time: 40.46 seconds\n",
      "Epoch [10/29], Loss: 4.1559, Test Acc:  7.68%, Time: 41.26 seconds\n",
      "Epoch [11/29], Loss: 4.0645, Test Acc:  5.15%, Time: 41.75 seconds\n",
      "Epoch [12/29], Loss: 3.9758, Test Acc:  8.71%, Time: 41.22 seconds\n",
      "Epoch [13/29], Loss: 3.8965, Test Acc: 11.56%, Time: 40.75 seconds\n",
      "Epoch [14/29], Loss: 3.8173, Test Acc:  4.83%, Time: 42.13 seconds\n",
      "Epoch [15/29], Loss: 3.7411, Test Acc:  8.79%, Time: 41.23 seconds\n",
      "Epoch [16/29], Loss: 3.6650, Test Acc: 15.58%, Time: 40.55 seconds\n",
      "Epoch [17/29], Loss: 3.5934, Test Acc:  8.16%, Time: 41.20 seconds\n",
      "Epoch [18/29], Loss: 3.5206, Test Acc:  9.09%, Time: 41.39 seconds\n",
      "Epoch [19/29], Loss: 3.4542, Test Acc: 10.31%, Time: 41.62 seconds\n",
      "Epoch [20/29], Loss: 3.3848, Test Acc: 10.82%, Time: 40.83 seconds\n",
      "Epoch [21/29], Loss: 3.3177, Test Acc: 11.03%, Time: 40.25 seconds\n",
      "Epoch [22/29], Loss: 3.2460, Test Acc: 13.12%, Time: 40.39 seconds\n",
      "Epoch [23/29], Loss: 3.1899, Test Acc:  7.66%, Time: 40.48 seconds\n",
      "Epoch [24/29], Loss: 3.1239, Test Acc:  7.27%, Time: 40.55 seconds\n",
      "Epoch [25/29], Loss: 3.0566, Test Acc: 11.00%, Time: 40.26 seconds\n",
      "Epoch [26/29], Loss: 2.9909, Test Acc:  8.96%, Time: 41.02 seconds\n",
      "Epoch [27/29], Loss: 2.9274, Test Acc: 10.48%, Time: 40.84 seconds\n",
      "Epoch [28/29], Loss: 2.8567, Test Acc:  6.98%, Time: 40.74 seconds\n",
      "Epoch [29/29], Loss: 2.8016, Test Acc:  9.51%, Time: 41.47 seconds\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.1558   \u001b[39m | \u001b[39m28.68    \u001b[39m | \u001b[39m408.3    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.0001459\u001b[39m | \u001b[39m59.61    \u001b[39m | \u001b[39m-4.784   \u001b[39m | \u001b[39m10.55    \u001b[39m | \u001b[39m4.151    \u001b[39m |\n",
      "Epoch [1/37], Loss: 5.1918, Test Acc:  2.11%, Time: 42.09 seconds\n",
      "Epoch [2/37], Loss: 4.8691, Test Acc:  1.82%, Time: 41.35 seconds\n",
      "Epoch [3/37], Loss: 4.5844, Test Acc:  4.75%, Time: 41.39 seconds\n",
      "Epoch [4/37], Loss: 4.3594, Test Acc:  3.36%, Time: 41.26 seconds\n",
      "Epoch [5/37], Loss: 4.1741, Test Acc: 12.87%, Time: 41.16 seconds\n",
      "Epoch [6/37], Loss: 4.0126, Test Acc:  2.81%, Time: 41.36 seconds\n",
      "Epoch [7/37], Loss: 3.8687, Test Acc:  9.71%, Time: 41.37 seconds\n",
      "Epoch [8/37], Loss: 3.7424, Test Acc:  6.12%, Time: 41.92 seconds\n",
      "Epoch [9/37], Loss: 3.6316, Test Acc:  8.96%, Time: 42.43 seconds\n",
      "Epoch [10/37], Loss: 3.5302, Test Acc:  6.87%, Time: 43.04 seconds\n",
      "Epoch [11/37], Loss: 3.4416, Test Acc:  3.06%, Time: 41.72 seconds\n",
      "Epoch [12/37], Loss: 3.3536, Test Acc:  6.13%, Time: 41.81 seconds\n",
      "Epoch [13/37], Loss: 3.2706, Test Acc: 11.83%, Time: 41.70 seconds\n",
      "Epoch [14/37], Loss: 3.1863, Test Acc:  9.15%, Time: 42.30 seconds\n",
      "Epoch [15/37], Loss: 3.1141, Test Acc:  4.06%, Time: 42.41 seconds\n",
      "Epoch [16/37], Loss: 3.0418, Test Acc:  5.59%, Time: 42.45 seconds\n",
      "Epoch [17/37], Loss: 2.9666, Test Acc:  8.26%, Time: 44.02 seconds\n",
      "Epoch [18/37], Loss: 2.8971, Test Acc:  8.88%, Time: 43.35 seconds\n",
      "Epoch [19/37], Loss: 2.8341, Test Acc:  9.34%, Time: 41.75 seconds\n",
      "Epoch [20/37], Loss: 2.7821, Test Acc:  4.87%, Time: 41.46 seconds\n",
      "Epoch [21/37], Loss: 2.7007, Test Acc:  7.48%, Time: 42.87 seconds\n",
      "Epoch [22/37], Loss: 2.6377, Test Acc:  5.89%, Time: 41.92 seconds\n",
      "Epoch [23/37], Loss: 2.5703, Test Acc: 13.03%, Time: 42.70 seconds\n",
      "Epoch [24/37], Loss: 2.5236, Test Acc:  5.76%, Time: 43.58 seconds\n",
      "Epoch [25/37], Loss: 2.4453, Test Acc: 23.61%, Time: 43.17 seconds\n",
      "Epoch [26/37], Loss: 2.3704, Test Acc:  9.52%, Time: 42.16 seconds\n",
      "Epoch [27/37], Loss: 2.3132, Test Acc: 13.94%, Time: 42.52 seconds\n",
      "Epoch [28/37], Loss: 2.2372, Test Acc:  9.75%, Time: 42.45 seconds\n",
      "Epoch [29/37], Loss: 2.2023, Test Acc: 16.00%, Time: 42.15 seconds\n",
      "Epoch [30/37], Loss: 2.1261, Test Acc:  7.75%, Time: 43.40 seconds\n",
      "Epoch [31/37], Loss: 2.0578, Test Acc:  7.31%, Time: 41.66 seconds\n",
      "Epoch [32/37], Loss: 1.9882, Test Acc: 14.83%, Time: 41.80 seconds\n",
      "Epoch [33/37], Loss: 1.9388, Test Acc: 23.97%, Time: 41.98 seconds\n",
      "Epoch [34/37], Loss: 1.8804, Test Acc: 13.63%, Time: 41.40 seconds\n",
      "Epoch [35/37], Loss: 1.7969, Test Acc:  8.01%, Time: 42.10 seconds\n",
      "Epoch [36/37], Loss: 1.7809, Test Acc: 20.24%, Time: 42.11 seconds\n",
      "Epoch [37/37], Loss: 1.7202, Test Acc:  5.83%, Time: 41.48 seconds\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.2397   \u001b[39m | \u001b[39m36.68    \u001b[39m | \u001b[39m490.2    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.0005647\u001b[39m | \u001b[39m50.47    \u001b[39m | \u001b[39m-2.318   \u001b[39m | \u001b[39m6.328    \u001b[39m | \u001b[39m3.987    \u001b[39m |\n",
      "Epoch [1/40], Loss: 5.2630, Test Acc:  2.52%, Time: 40.10 seconds\n",
      "Epoch [2/40], Loss: 5.1616, Test Acc:  3.20%, Time: 40.29 seconds\n",
      "Epoch [3/40], Loss: 5.0508, Test Acc:  3.60%, Time: 39.90 seconds\n",
      "Epoch [4/40], Loss: 4.9395, Test Acc:  3.44%, Time: 39.21 seconds\n",
      "Epoch [5/40], Loss: 4.8352, Test Acc:  6.22%, Time: 40.25 seconds\n",
      "Epoch [6/40], Loss: 4.7361, Test Acc: 10.26%, Time: 40.20 seconds\n",
      "Epoch [7/40], Loss: 4.6428, Test Acc:  4.14%, Time: 39.41 seconds\n",
      "Epoch [8/40], Loss: 4.5530, Test Acc:  6.56%, Time: 40.28 seconds\n",
      "Epoch [9/40], Loss: 4.4708, Test Acc:  6.85%, Time: 39.18 seconds\n",
      "Epoch [10/40], Loss: 4.3851, Test Acc:  8.95%, Time: 39.53 seconds\n",
      "Epoch [11/40], Loss: 4.3072, Test Acc:  5.77%, Time: 40.25 seconds\n",
      "Epoch [12/40], Loss: 4.2306, Test Acc:  5.38%, Time: 40.58 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m optimizer = BayesianOptimization(\n\u001b[32m      2\u001b[39m     f=optimize_kan,\n\u001b[32m      3\u001b[39m     pbounds=pbounds,\n\u001b[32m      4\u001b[39m     random_state=\u001b[32m38\u001b[39m,\n\u001b[32m      5\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 8 random warmâ€‘ups + 10 guided iterations\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ best combo so far â†’\u001b[39m\u001b[33m\"\u001b[39m, optimizer.max)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:338\u001b[39m, in \u001b[36mBayesianOptimization.maximize\u001b[39m\u001b[34m(self, init_points, n_iter)\u001b[39m\n\u001b[32m    336\u001b[39m     x_probe = \u001b[38;5;28mself\u001b[39m.suggest()\n\u001b[32m    337\u001b[39m     iteration += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration > \u001b[32m0\u001b[39m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_bounds(\u001b[38;5;28mself\u001b[39m._bounds_transformer.transform(\u001b[38;5;28mself\u001b[39m._space))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:270\u001b[39m, in \u001b[36mBayesianOptimization.probe\u001b[39m\u001b[34m(self, params, lazy)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m._queue.append(params)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch(Events.OPTIMIZATION_STEP)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/target_space.py:418\u001b[39m, in \u001b[36mTargetSpace.probe\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    416\u001b[39m     error_msg = \u001b[33m\"\u001b[39m\u001b[33mNo target function has been provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m.register(x, target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36moptimize_kan\u001b[39m\u001b[34m(epochs, kan_inner, kan_outer, spline_cp, spline_deg, range_min, range_max, lr)\u001b[39m\n\u001b[32m     86\u001b[39m lr          = \u001b[38;5;28mfloat\u001b[39m(lr)\n\u001b[32m     88\u001b[39m model = KANCNN(\n\u001b[32m     89\u001b[39m     kan_inner=kan_inner,\n\u001b[32m     90\u001b[39m     kan_outer=kan_outer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     range_max=range_max\n\u001b[32m     95\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m val_acc = \u001b[43mtrain_kan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# BayesOpt maximizes the returned value\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m val_acc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_kan\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, lr)\u001b[39m\n\u001b[32m     32\u001b[39m loss.backward()\n\u001b[32m     33\u001b[39m optim.step()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m acc = \u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m loss_sum += loss.item() * x.size(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m acc_sum  += acc * x.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_kan,\n",
    "    pbounds=pbounds,\n",
    "    random_state=38,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 8 random warmâ€‘ups + 10 guided iterations\n",
    "optimizer.maximize(init_points=6, n_iter=25)\n",
    "\n",
    "print(\"ðŸš€ best combo so far â†’\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
