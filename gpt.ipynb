{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, Union, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip, ToImage,\n",
    "    ToDtype, Normalize, Lambda                    # ⬅ new\n",
    ")\n",
    "from torchvision.utils import make_grid\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# -----------------------  constants  ---------------------------------\n",
    "SEED               = 42\n",
    "N_EPOCHS           = 10\n",
    "BATCH_SIZE         = 128\n",
    "IMAGE_SIZE         = (64, 64)            # Tiny‑ImageNet native size\n",
    "LEARNING_RATE      = 3e-4\n",
    "WEIGHT_DECAY       = 1e-5\n",
    "RUN_DIR            = Path(\"runs/tinyimagenet_cnn_vs_kan\").resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"➡️  Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------  utils  -------------------------------------\n",
    "def _force_rgb(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure 3‑channel RGB.\n",
    "\n",
    "    • If RGBA ➜ drop alpha.\n",
    "    • If grayscale ➜ replicate to 3 channels.\n",
    "    \"\"\"\n",
    "    if img.shape[0] == 4:        # RGBA → RGB\n",
    "        return img[:3]\n",
    "    if img.shape[0] == 1:        # Gray → RGB by tiling\n",
    "        return img.repeat(3, 1, 1)\n",
    "    return img                   # already RGB\n",
    "\n",
    "def get_dataloaders(batch_size:int=BATCH_SIZE,\n",
    "                    image_size:Tuple[int,int]=IMAGE_SIZE,\n",
    "                    dataset_name:str=\"zh-plus/tiny-imagenet\"):\n",
    "    \"\"\"\n",
    "    Return train/val/test DataLoaders for Tiny‑ImageNet.\n",
    "\n",
    "    Splits:\n",
    "        • train  – 100 000 imgs\n",
    "        • valid  – 10 000 imgs (used here as held‑out **test**)\n",
    "    We further split 10 % of *train* into an internal validation set.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    # carve INTERNAL val from train\n",
    "    split = ds[\"train\"].train_test_split(\n",
    "        test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n",
    "    train_ds = split[\"train\"]\n",
    "    val_ds   = split[\"test\"]\n",
    "    test_ds  = ds[\"valid\"]                 # official validation → test\n",
    "\n",
    "    # ImageNet normalisation stats\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    tfms = Compose([\n",
    "        ToImage(),                       # PIL → (C,H,W) uint8 tensor\n",
    "        Lambda(_force_rgb),              # ⬅ squash grayscale → RGB\n",
    "        RandomHorizontalFlip(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    def add_tfms(example):\n",
    "        example[\"image\"] = tfms(example[\"image\"])\n",
    "        return example\n",
    "\n",
    "    for split_ds in (train_ds, val_ds, test_ds):\n",
    "        split_ds.set_transform(add_tfms)\n",
    "\n",
    "    loader_cfg = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "    val_loader   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "    test_loader  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Tiny‑ImageNet‑sized baseline CNN\n",
    "    • Input : (B, 3, 64, 64)\n",
    "    • Output: (B, num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple[int, int, int] = (3, 64, 64),\n",
    "        num_classes: int = 200,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "\n",
    "        # ─────────── feature extractor ───────────\n",
    "        self.features = nn.Sequential(\n",
    "            # 64×64 → 64×64\n",
    "            nn.Conv2d(C_in, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            # 64×64 → 32×32\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 32×32 → 16×16\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 16×16 → 8×8\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 8×8 → 4×4\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 4×4 → 1×1 (global pooling)\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "        )\n",
    "\n",
    "        # ─────────── classifier ───────────\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),               # (B, 512, 1, 1) → (B, 512)\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight‑init\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # ────────────────────────────────────────────\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images : torch.Size([128, 3, 64, 64]) torch.float32 -2.1179039478302 255.0\n",
      "labels : tensor([183,  22, 183,  72,  61,  96, 151, 120, 195, 164,   5, 143, 116, 146,\n",
      "        135, 197, 170, 132,  48,  16]) torch.int64\n",
      "# unique classes in batch: 93\n"
     ]
    }
   ],
   "source": [
    "batch   = next(iter(train_loader))\n",
    "images  = batch[\"image\"]           \n",
    "labels  = batch[\"label\"]          \n",
    "\n",
    "print(\"images :\", images.shape, images.dtype, images.min().item(), images.max().item())\n",
    "print(\"labels :\", labels[:20], labels.dtype)\n",
    "print(\"# unique classes in batch:\", len(torch.unique(labels)))\n",
    "\n",
    "# sanity checks you should see\n",
    "#   • dtype: torch.float32\n",
    "#   • min/max roughly -2 ↔ +2      (ImageNet mean/std normalisation)\n",
    "#   • label dtype: torch.int64\n",
    "#   • at least a handful of different label values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss_fn, optim, device=device):\n",
    "    model.train()\n",
    "    tot_loss = tot_correct = 0\n",
    "    for batch in loader:\n",
    "        x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss   = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        tot_loss    += loss.item() * x.size(0)\n",
    "        tot_correct += (logits.argmax(1) == y).sum().item()\n",
    "    n = len(loader.dataset)\n",
    "    return tot_loss/n, tot_correct/n          # avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device=device):\n",
    "    model.eval()\n",
    "    tot_loss = tot_correct = 0\n",
    "    for batch in loader:\n",
    "        x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        logits = model(x)\n",
    "        loss   = loss_fn(logits, y)\n",
    "        tot_loss    += loss.item() * x.size(0)\n",
    "        tot_correct += (logits.argmax(1) == y).sum().item()\n",
    "    n = len(loader.dataset)\n",
    "    return tot_loss/n, tot_correct/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_cnn(lr, dropout, weight_decay,\n",
    "                   train_loader=train_loader, val_loader=val_loader,\n",
    "                   n_epochs=30):\n",
    "    lr, dropout, weight_decay = float(lr), float(dropout), float(weight_decay)\n",
    "    model     = BaselineCNN(dropout=dropout).to(device)\n",
    "    optim     = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn   = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"[trial] lr={lr:.1e}  dr={dropout:.2f}  wd={weight_decay:.1e}\")\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        tr_loss, tr_acc = train(model, train_loader, loss_fn, optim)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, loss_fn)\n",
    "        print(f\"Epoch {epoch:02d} • \"\n",
    "              f\"train {tr_acc*100:5.2f}% / val {va_acc*100:5.2f}% | \"\n",
    "              f\"loss {tr_loss:.3f}/{va_loss:.3f}\")\n",
    "    return va_acc                                # return best/final as you wish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  dropout  |    lr     | weight... |\n",
      "-------------------------------------------------------------\n",
      "[trial] lr=2.9e-03  dr=0.15  wd=7.3e-04\n",
      "Epoch 01 • train  0.62% / val  1.16% | loss 5.291/5.170\n",
      "Epoch 02 • train  1.90% / val  0.97% | loss 5.052/5.273\n",
      "Epoch 03 • train  3.36% / val  1.30% | loss 4.853/5.284\n",
      "Epoch 04 • train  5.39% / val  2.52% | loss 4.634/5.183\n",
      "Epoch 05 • train  7.31% / val  1.08% | loss 4.453/6.082\n",
      "Epoch 06 • train  9.32% / val  1.40% | loss 4.289/5.979\n",
      "Epoch 07 • train 10.85% / val  1.09% | loss 4.152/5.744\n",
      "Epoch 08 • train 12.23% / val  1.99% | loss 4.043/5.698\n",
      "Epoch 09 • train 13.26% / val  2.37% | loss 3.964/5.612\n",
      "Epoch 10 • train 14.04% / val  1.05% | loss 3.909/5.982\n",
      "Epoch 11 • train 14.88% / val  1.22% | loss 3.858/5.950\n",
      "Epoch 12 • train 15.25% / val  1.70% | loss 3.832/5.719\n",
      "Epoch 13 • train 15.63% / val  1.45% | loss 3.800/5.850\n",
      "Epoch 14 • train 16.23% / val  2.54% | loss 3.774/5.634\n",
      "Epoch 15 • train 16.58% / val  3.88% | loss 3.754/5.232\n",
      "Epoch 16 • train 16.69% / val  2.51% | loss 3.743/5.401\n",
      "Epoch 17 • train 17.05% / val  1.75% | loss 3.716/5.696\n",
      "Epoch 18 • train 17.33% / val  1.05% | loss 3.704/5.852\n",
      "Epoch 19 • train 17.62% / val  1.81% | loss 3.693/5.632\n",
      "Epoch 20 • train 17.75% / val  1.44% | loss 3.679/5.781\n",
      "Epoch 21 • train 17.68% / val  1.00% | loss 3.670/5.732\n",
      "Epoch 22 • train 18.14% / val  7.07% | loss 3.650/4.540\n",
      "Epoch 23 • train 18.32% / val  1.63% | loss 3.635/5.894\n",
      "Epoch 24 • train 18.56% / val  0.94% | loss 3.626/5.891\n",
      "Epoch 25 • train 18.78% / val  1.52% | loss 3.614/6.032\n",
      "Epoch 26 • train 18.79% / val  1.48% | loss 3.607/5.859\n",
      "Epoch 27 • train 18.94% / val  1.43% | loss 3.594/5.914\n",
      "Epoch 28 • train 19.05% / val  2.60% | loss 3.590/5.879\n",
      "Epoch 29 • train 19.31% / val  2.94% | loss 3.583/5.605\n",
      "Epoch 30 • train 19.49% / val  2.47% | loss 3.568/5.512\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.0247   \u001b[39m | \u001b[39m0.1498   \u001b[39m | \u001b[39m0.002857 \u001b[39m | \u001b[39m0.0007323\u001b[39m |\n",
      "[trial] lr=5.5e-04  dr=0.24  wd=1.6e-04\n",
      "Epoch 01 • train  0.48% / val  0.52% | loss 5.337/5.294\n",
      "Epoch 02 • train  1.06% / val  0.83% | loss 5.184/5.297\n",
      "Epoch 03 • train  3.15% / val  1.75% | loss 4.907/5.288\n",
      "Epoch 04 • train  6.76% / val  2.06% | loss 4.504/5.666\n",
      "Epoch 05 • train 11.78% / val  3.31% | loss 4.107/5.218\n",
      "Epoch 06 • train 15.76% / val  2.95% | loss 3.801/5.535\n",
      "Epoch 07 • train 19.04% / val  9.64% | loss 3.589/4.336\n",
      "Epoch 08 • train 21.73% / val  8.66% | loss 3.417/4.444\n",
      "Epoch 09 • train 24.21% / val  9.00% | loss 3.277/4.448\n",
      "Epoch 10 • train 26.09% / val  5.18% | loss 3.160/5.375\n",
      "Epoch 11 • train 28.34% / val  5.98% | loss 3.051/5.056\n",
      "Epoch 12 • train 29.99% / val  7.09% | loss 2.950/5.096\n",
      "Epoch 13 • train 31.51% / val 10.11% | loss 2.864/4.673\n",
      "Epoch 14 • train 32.92% / val  3.50% | loss 2.782/5.944\n",
      "Epoch 15 • train 34.35% / val 10.65% | loss 2.705/4.444\n",
      "Epoch 16 • train 35.87% / val  7.95% | loss 2.631/4.764\n",
      "Epoch 17 • train 37.21% / val  3.37% | loss 2.558/6.210\n",
      "Epoch 18 • train 38.22% / val  8.03% | loss 2.507/4.977\n",
      "Epoch 19 • train 39.39% / val  4.70% | loss 2.440/5.821\n",
      "Epoch 20 • train 40.71% / val  7.82% | loss 2.378/4.834\n",
      "Epoch 21 • train 41.97% / val  4.17% | loss 2.316/5.641\n",
      "Epoch 22 • train 42.92% / val 14.00% | loss 2.257/4.180\n",
      "Epoch 23 • train 44.26% / val  9.30% | loss 2.201/4.812\n",
      "Epoch 24 • train 45.36% / val  9.18% | loss 2.146/4.748\n",
      "Epoch 25 • train 46.48% / val  5.82% | loss 2.082/5.674\n",
      "Epoch 26 • train 47.63% / val  4.40% | loss 2.029/5.835\n",
      "Epoch 27 • train 48.56% / val 13.92% | loss 1.981/4.272\n",
      "Epoch 28 • train 49.77% / val 15.33% | loss 1.919/4.091\n",
      "Epoch 29 • train 51.07% / val 12.97% | loss 1.865/4.313\n",
      "Epoch 30 • train 51.71% / val  7.88% | loss 1.829/4.963\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.0788   \u001b[39m | \u001b[35m0.2395   \u001b[39m | \u001b[35m0.0005525\u001b[39m | \u001b[35m0.0001568\u001b[39m |\n",
      "[trial] lr=2.6e-03  dr=0.02  wd=6.0e-04\n",
      "Epoch 01 • train  1.28% / val  0.74% | loss 5.185/5.321\n",
      "Epoch 02 • train  2.53% / val  1.41% | loss 4.962/5.319\n",
      "Epoch 03 • train  4.89% / val  3.06% | loss 4.685/4.944\n",
      "Epoch 04 • train  8.00% / val  3.92% | loss 4.402/4.923\n",
      "Epoch 05 • train 11.17% / val  2.32% | loss 4.155/5.571\n",
      "Epoch 06 • train 13.84% / val  4.34% | loss 3.955/4.935\n",
      "Epoch 07 • train 15.68% / val  5.63% | loss 3.821/4.751\n",
      "Epoch 08 • train 17.21% / val  4.00% | loss 3.723/5.168\n",
      "Epoch 09 • train 18.35% / val  4.40% | loss 3.641/5.266\n",
      "Epoch 10 • train 19.56% / val  1.53% | loss 3.580/5.946\n",
      "Epoch 11 • train 20.31% / val  2.83% | loss 3.521/5.776\n",
      "Epoch 12 • train 20.92% / val  4.40% | loss 3.473/5.569\n",
      "Epoch 13 • train 21.62% / val 10.94% | loss 3.439/4.299\n",
      "Epoch 14 • train 22.50% / val  6.12% | loss 3.397/4.862\n",
      "Epoch 15 • train 22.77% / val  2.97% | loss 3.374/5.451\n",
      "Epoch 16 • train 23.45% / val  3.72% | loss 3.345/5.419\n",
      "Epoch 17 • train 23.59% / val 17.67% | loss 3.331/3.730\n",
      "Epoch 18 • train 24.03% / val  4.63% | loss 3.302/5.186\n",
      "Epoch 19 • train 24.42% / val  1.95% | loss 3.281/6.568\n",
      "Epoch 20 • train 24.65% / val  6.38% | loss 3.262/5.120\n",
      "Epoch 21 • train 25.00% / val  5.84% | loss 3.253/4.920\n",
      "Epoch 22 • train 25.27% / val  1.34% | loss 3.231/6.341\n",
      "Epoch 23 • train 25.55% / val  3.17% | loss 3.221/5.681\n",
      "Epoch 24 • train 26.02% / val  1.87% | loss 3.202/6.419\n",
      "Epoch 25 • train 25.85% / val  2.66% | loss 3.200/5.751\n",
      "Epoch 26 • train 26.21% / val  2.02% | loss 3.189/6.497\n",
      "Epoch 27 • train 26.50% / val  2.62% | loss 3.180/6.064\n",
      "Epoch 28 • train 26.25% / val  2.35% | loss 3.174/6.066\n",
      "Epoch 29 • train 26.69% / val  1.09% | loss 3.161/6.659\n",
      "Epoch 30 • train 26.63% / val  2.02% | loss 3.154/6.253\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.0202   \u001b[39m | \u001b[39m0.02323  \u001b[39m | \u001b[39m0.002612 \u001b[39m | \u001b[39m0.0006015\u001b[39m |\n",
      "[trial] lr=1.6e-04  dr=0.28  wd=9.7e-04\n",
      "Epoch 01 • train  0.50% / val  0.53% | loss 5.364/5.298\n",
      "Epoch 02 • train  0.90% / val  0.90% | loss 5.245/5.209\n",
      "Epoch 03 • train  1.65% / val  1.38% | loss 5.092/5.179\n",
      "Epoch 04 • train  3.56% / val  1.58% | loss 4.868/5.147\n",
      "Epoch 05 • train  6.53% / val  2.58% | loss 4.551/5.165\n",
      "Epoch 06 • train 10.03% / val  1.94% | loss 4.235/5.548\n",
      "Epoch 07 • train 13.39% / val  5.31% | loss 3.978/4.781\n",
      "Epoch 08 • train 16.14% / val  4.32% | loss 3.777/4.931\n",
      "Epoch 09 • train 18.50% / val  4.50% | loss 3.629/4.904\n",
      "Epoch 10 • train 20.42% / val  3.16% | loss 3.506/5.244\n",
      "Epoch 11 • train 22.38% / val  5.27% | loss 3.392/4.887\n",
      "Epoch 12 • train 24.04% / val 13.10% | loss 3.299/3.958\n",
      "Epoch 13 • train 25.56% / val  4.74% | loss 3.212/4.928\n",
      "Epoch 14 • train 26.99% / val  6.64% | loss 3.126/4.680\n",
      "Epoch 15 • train 28.40% / val  4.28% | loss 3.053/5.170\n",
      "Epoch 16 • train 29.68% / val  6.03% | loss 2.983/5.027\n",
      "Epoch 17 • train 30.57% / val  8.18% | loss 2.924/4.537\n",
      "Epoch 18 • train 32.07% / val  9.20% | loss 2.854/4.375\n",
      "Epoch 19 • train 32.94% / val  7.36% | loss 2.807/4.665\n",
      "Epoch 20 • train 34.12% / val  3.29% | loss 2.743/5.461\n",
      "Epoch 21 • train 35.07% / val  7.34% | loss 2.686/4.839\n",
      "Epoch 22 • train 36.12% / val  8.19% | loss 2.626/4.750\n",
      "Epoch 23 • train 37.17% / val 19.89% | loss 2.575/3.553\n",
      "Epoch 24 • train 38.26% / val  7.07% | loss 2.516/4.802\n",
      "Epoch 25 • train 39.43% / val  9.67% | loss 2.460/4.457\n",
      "Epoch 26 • train 40.39% / val  8.83% | loss 2.409/4.597\n",
      "Epoch 27 • train 41.46% / val  5.56% | loss 2.356/5.322\n",
      "Epoch 28 • train 42.52% / val 12.64% | loss 2.297/4.140\n",
      "Epoch 29 • train 43.72% / val  5.80% | loss 2.239/5.160\n",
      "Epoch 30 • train 44.75% / val  9.41% | loss 2.183/4.554\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.0941   \u001b[39m | \u001b[35m0.2832   \u001b[39m | \u001b[35m0.0001597\u001b[39m | \u001b[35m0.0009699\u001b[39m |\n",
      "[trial] lr=7.2e-04  dr=0.33  wd=1.8e-04\n",
      "Epoch 01 • train  0.50% / val  0.51% | loss 5.336/5.298\n",
      "Epoch 02 • train  0.78% / val  1.64% | loss 5.251/5.129\n",
      "Epoch 03 • train  1.82% / val  1.75% | loss 5.043/5.012\n",
      "Epoch 04 • train  3.63% / val  1.72% | loss 4.807/5.406\n",
      "Epoch 05 • train  8.12% / val  3.13% | loss 4.364/5.587\n",
      "Epoch 06 • train 12.89% / val  1.85% | loss 3.986/6.078\n",
      "Epoch 07 • train 16.46% / val  7.27% | loss 3.735/4.668\n",
      "Epoch 08 • train 19.00% / val  1.88% | loss 3.568/6.331\n",
      "Epoch 09 • train 21.32% / val 17.68% | loss 3.427/3.672\n",
      "Epoch 10 • train 23.43% / val  0.83% | loss 3.310/7.527\n",
      "Epoch 11 • train 24.80% / val  1.98% | loss 3.217/6.601\n",
      "Epoch 12 • train 26.30% / val  1.57% | loss 3.122/6.949\n",
      "Epoch 13 • train 27.77% / val  5.81% | loss 3.050/5.204\n",
      "Epoch 14 • train 29.16% / val  2.78% | loss 2.977/5.907\n",
      "Epoch 15 • train 30.50% / val  1.92% | loss 2.908/7.033\n",
      "Epoch 16 • train 31.67% / val  2.25% | loss 2.849/6.171\n",
      "Epoch 17 • train 32.45% / val  2.01% | loss 2.798/6.864\n",
      "Epoch 18 • train 33.83% / val  1.74% | loss 2.741/6.769\n",
      "Epoch 19 • train 34.50% / val  2.41% | loss 2.683/6.506\n",
      "Epoch 20 • train 35.63% / val  2.49% | loss 2.638/6.521\n",
      "Epoch 21 • train 36.27% / val  3.11% | loss 2.595/5.936\n",
      "Epoch 22 • train 37.28% / val  1.70% | loss 2.551/6.738\n",
      "Epoch 23 • train 38.21% / val 12.40% | loss 2.497/4.433\n",
      "Epoch 24 • train 38.88% / val  1.64% | loss 2.461/6.823\n",
      "Epoch 25 • train 39.71% / val  2.78% | loss 2.410/6.619\n",
      "Epoch 26 • train 40.72% / val  3.86% | loss 2.368/5.970\n",
      "Epoch 27 • train 41.28% / val  2.34% | loss 2.333/6.122\n",
      "Epoch 28 • train 42.01% / val  1.71% | loss 2.289/7.310\n",
      "Epoch 29 • train 42.88% / val  6.28% | loss 2.254/5.275\n",
      "Epoch 30 • train 43.81% / val  2.82% | loss 2.210/6.331\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.0282   \u001b[39m | \u001b[39m0.333    \u001b[39m | \u001b[39m0.0007158\u001b[39m | \u001b[39m0.0001826\u001b[39m |\n",
      "[trial] lr=2.4e-03  dr=0.28  wd=1.9e-04\n",
      "Epoch 01 • train  0.48% / val  0.52% | loss 5.336/5.298\n",
      "Epoch 02 • train  0.73% / val  1.26% | loss 5.237/5.176\n",
      "Epoch 03 • train  2.03% / val  0.93% | loss 5.036/5.275\n",
      "Epoch 04 • train  3.18% / val  1.04% | loss 4.870/5.504\n",
      "Epoch 05 • train  6.18% / val  3.45% | loss 4.563/4.992\n",
      "Epoch 06 • train  9.70% / val  1.51% | loss 4.256/6.052\n",
      "Epoch 07 • train 12.50% / val  3.56% | loss 4.028/5.556\n",
      "Epoch 08 • train 14.43% / val  3.77% | loss 3.878/5.665\n",
      "Epoch 09 • train 16.16% / val  4.99% | loss 3.765/5.108\n",
      "Epoch 10 • train 17.66% / val  2.34% | loss 3.672/6.538\n",
      "Epoch 11 • train 18.73% / val  3.70% | loss 3.603/5.363\n",
      "Epoch 12 • train 19.93% / val  0.82% | loss 3.532/7.489\n",
      "Epoch 13 • train 20.73% / val  3.61% | loss 3.493/5.210\n",
      "Epoch 14 • train 21.56% / val  3.91% | loss 3.437/5.271\n",
      "Epoch 15 • train 22.61% / val  7.93% | loss 3.389/4.615\n",
      "Epoch 16 • train 23.13% / val  3.94% | loss 3.347/5.404\n",
      "Epoch 17 • train 23.76% / val  5.57% | loss 3.306/4.993\n",
      "Epoch 18 • train 24.38% / val  2.55% | loss 3.277/5.738\n",
      "Epoch 19 • train 24.75% / val  3.96% | loss 3.254/5.316\n",
      "Epoch 20 • train 25.46% / val  6.25% | loss 3.221/4.983\n",
      "Epoch 21 • train 25.94% / val  7.20% | loss 3.193/4.597\n",
      "Epoch 22 • train 26.25% / val 12.62% | loss 3.172/4.077\n",
      "Epoch 23 • train 26.78% / val  2.70% | loss 3.150/5.844\n",
      "Epoch 24 • train 27.08% / val 12.25% | loss 3.132/4.267\n",
      "Epoch 25 • train 27.33% / val 13.19% | loss 3.111/4.028\n",
      "Epoch 26 • train 27.73% / val  9.75% | loss 3.099/4.513\n",
      "Epoch 27 • train 28.04% / val  4.78% | loss 3.076/5.233\n",
      "Epoch 28 • train 28.46% / val 10.53% | loss 3.065/4.343\n",
      "Epoch 29 • train 28.80% / val  4.38% | loss 3.041/5.273\n",
      "Epoch 30 • train 29.05% / val  6.37% | loss 3.027/5.254\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.0637   \u001b[39m | \u001b[39m0.2827   \u001b[39m | \u001b[39m0.002392 \u001b[39m | \u001b[39m0.0001945\u001b[39m |\n",
      "[trial] lr=5.1e-04  dr=0.28  wd=6.2e-04\n",
      "Epoch 01 • train  0.50% / val  0.52% | loss 5.330/5.296\n",
      "Epoch 02 • train  1.22% / val  1.07% | loss 5.155/5.371\n",
      "Epoch 03 • train  3.97% / val  2.75% | loss 4.829/5.053\n",
      "Epoch 04 • train  8.65% / val  2.30% | loss 4.343/5.369\n",
      "Epoch 05 • train 13.21% / val  3.31% | loss 3.975/5.306\n",
      "Epoch 06 • train 16.70% / val  3.39% | loss 3.737/5.839\n",
      "Epoch 07 • train 19.39% / val  3.47% | loss 3.556/5.307\n",
      "Epoch 08 • train 21.72% / val  5.59% | loss 3.422/5.206\n",
      "Epoch 09 • train 23.57% / val  4.33% | loss 3.310/5.126\n",
      "Epoch 10 • train 25.22% / val  3.65% | loss 3.211/5.428\n",
      "Epoch 11 • train 26.72% / val  3.95% | loss 3.119/5.384\n",
      "Epoch 12 • train 28.22% / val  4.00% | loss 3.045/5.544\n",
      "Epoch 13 • train 29.16% / val  2.67% | loss 2.986/6.059\n",
      "Epoch 14 • train 30.53% / val  4.63% | loss 2.916/5.463\n",
      "Epoch 15 • train 31.35% / val  4.40% | loss 2.870/5.894\n",
      "Epoch 16 • train 32.38% / val  5.36% | loss 2.816/5.038\n",
      "Epoch 17 • train 33.29% / val  2.47% | loss 2.773/6.571\n",
      "Epoch 18 • train 34.02% / val  3.51% | loss 2.726/5.839\n",
      "Epoch 19 • train 34.93% / val  6.19% | loss 2.685/5.022\n",
      "Epoch 20 • train 35.42% / val  5.83% | loss 2.650/5.402\n",
      "Epoch 21 • train 36.42% / val  2.90% | loss 2.608/6.219\n",
      "Epoch 22 • train 37.02% / val  2.97% | loss 2.568/6.394\n",
      "Epoch 23 • train 37.97% / val 10.28% | loss 2.525/4.438\n",
      "Epoch 24 • train 38.22% / val  6.03% | loss 2.499/5.532\n",
      "Epoch 25 • train 39.16% / val  6.37% | loss 2.468/5.159\n",
      "Epoch 26 • train 39.63% / val  3.21% | loss 2.435/6.179\n",
      "Epoch 27 • train 40.43% / val  3.81% | loss 2.395/5.623\n",
      "Epoch 28 • train 40.88% / val  2.48% | loss 2.376/6.157\n",
      "Epoch 29 • train 41.46% / val  6.04% | loss 2.347/5.348\n",
      "Epoch 30 • train 42.02% / val  2.78% | loss 2.311/6.773\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.0278   \u001b[39m | \u001b[39m0.2832   \u001b[39m | \u001b[39m0.000505 \u001b[39m | \u001b[39m0.0006227\u001b[39m |\n",
      "[trial] lr=5.3e-04  dr=0.24  wd=9.9e-05\n",
      "Epoch 01 • train  0.85% / val  1.67% | loss 5.282/5.126\n",
      "Epoch 02 • train  2.42% / val  1.20% | loss 4.997/5.302\n",
      "Epoch 03 • train  6.23% / val  3.04% | loss 4.586/5.083\n",
      "Epoch 04 • train 10.82% / val  3.06% | loss 4.183/5.177\n",
      "Epoch 05 • train 14.94% / val  4.55% | loss 3.865/5.470\n",
      "Epoch 06 • train 18.53% / val  5.74% | loss 3.632/4.780\n",
      "Epoch 07 • train 21.25% / val  2.39% | loss 3.457/5.697\n",
      "Epoch 08 • train 23.59% / val  5.19% | loss 3.311/4.974\n",
      "Epoch 09 • train 25.96% / val  4.63% | loss 3.182/5.349\n",
      "Epoch 10 • train 27.75% / val  4.53% | loss 3.066/5.496\n",
      "Epoch 11 • train 29.65% / val 14.92% | loss 2.964/3.952\n",
      "Epoch 12 • train 31.33% / val  6.66% | loss 2.878/5.147\n",
      "Epoch 13 • train 32.93% / val  9.75% | loss 2.790/4.473\n",
      "Epoch 14 • train 34.44% / val 10.06% | loss 2.706/4.567\n",
      "Epoch 15 • train 35.86% / val 10.49% | loss 2.635/4.426\n",
      "Epoch 16 • train 37.14% / val  9.76% | loss 2.562/4.486\n",
      "Epoch 17 • train 38.48% / val  7.88% | loss 2.495/4.832\n",
      "Epoch 18 • train 39.99% / val 11.86% | loss 2.420/4.282\n",
      "Epoch 19 • train 41.14% / val 11.10% | loss 2.356/4.461\n",
      "Epoch 20 • train 42.42% / val 18.81% | loss 2.290/3.753\n",
      "Epoch 21 • train 43.88% / val 12.55% | loss 2.220/4.549\n",
      "Epoch 22 • train 44.85% / val 16.36% | loss 2.167/4.008\n",
      "Epoch 23 • train 46.21% / val 10.02% | loss 2.104/4.625\n",
      "Epoch 24 • train 47.40% / val  7.53% | loss 2.046/5.146\n",
      "Epoch 25 • train 48.56% / val 25.43% | loss 1.979/3.319\n",
      "Epoch 26 • train 49.77% / val 19.10% | loss 1.916/3.812\n",
      "Epoch 27 • train 50.86% / val  9.17% | loss 1.858/5.037\n",
      "Epoch 28 • train 52.49% / val  9.42% | loss 1.793/4.861\n",
      "Epoch 29 • train 53.53% / val  6.75% | loss 1.737/5.646\n",
      "Epoch 30 • train 54.75% / val 15.92% | loss 1.686/4.182\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m0.1592   \u001b[39m | \u001b[35m0.2394   \u001b[39m | \u001b[35m0.0005292\u001b[39m | \u001b[35m9.919e-05\u001b[39m |\n",
      "[trial] lr=5.2e-04  dr=0.24  wd=4.1e-05\n",
      "Epoch 01 • train  0.57% / val  1.24% | loss 5.321/5.195\n",
      "Epoch 02 • train  1.84% / val  1.43% | loss 5.067/5.245\n",
      "Epoch 03 • train  4.24% / val  3.21% | loss 4.778/5.029\n",
      "Epoch 04 • train  8.33% / val  2.09% | loss 4.377/5.493\n",
      "Epoch 05 • train 13.01% / val  3.39% | loss 4.017/5.388\n",
      "Epoch 06 • train 16.64% / val  6.30% | loss 3.755/4.679\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m pbounds = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1e-4\u001b[39m, \u001b[32m3e-3\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m0.0\u001b[39m, \u001b[32m0.4\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1e-6\u001b[39m, \u001b[32m1e-3\u001b[39m)}\n\u001b[32m      2\u001b[39m bo = BayesianOptimization(\n\u001b[32m      3\u001b[39m     f=\u001b[38;5;28;01mlambda\u001b[39;00m lr, dropout, weight_decay: train_eval_cnn(lr, dropout, weight_decay),\n\u001b[32m      4\u001b[39m     pbounds=pbounds, random_state=\u001b[32m42\u001b[39m, verbose=\u001b[32m2\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mbo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbest →\u001b[39m\u001b[33m\"\u001b[39m, bo.max)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:338\u001b[39m, in \u001b[36mBayesianOptimization.maximize\u001b[39m\u001b[34m(self, init_points, n_iter)\u001b[39m\n\u001b[32m    336\u001b[39m     x_probe = \u001b[38;5;28mself\u001b[39m.suggest()\n\u001b[32m    337\u001b[39m     iteration += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration > \u001b[32m0\u001b[39m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_bounds(\u001b[38;5;28mself\u001b[39m._bounds_transformer.transform(\u001b[38;5;28mself\u001b[39m._space))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:270\u001b[39m, in \u001b[36mBayesianOptimization.probe\u001b[39m\u001b[34m(self, params, lazy)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m._queue.append(params)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch(Events.OPTIMIZATION_STEP)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/target_space.py:418\u001b[39m, in \u001b[36mTargetSpace.probe\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    416\u001b[39m     error_msg = \u001b[33m\"\u001b[39m\u001b[33mNo target function has been provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m.register(x, target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(lr, dropout, weight_decay)\u001b[39m\n\u001b[32m      1\u001b[39m pbounds = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1e-4\u001b[39m, \u001b[32m3e-3\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m0.0\u001b[39m, \u001b[32m0.4\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[32m1e-6\u001b[39m, \u001b[32m1e-3\u001b[39m)}\n\u001b[32m      2\u001b[39m bo = BayesianOptimization(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     f=\u001b[38;5;28;01mlambda\u001b[39;00m lr, dropout, weight_decay: \u001b[43mtrain_eval_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      4\u001b[39m     pbounds=pbounds, random_state=\u001b[32m42\u001b[39m, verbose=\u001b[32m2\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m bo.maximize(init_points=\u001b[32m5\u001b[39m, n_iter=\u001b[32m25\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbest →\u001b[39m\u001b[33m\"\u001b[39m, bo.max)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_eval_cnn\u001b[39m\u001b[34m(lr, dropout, weight_decay, train_loader, val_loader, n_epochs)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[trial] lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  dr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  wd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_epochs+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     tr_loss, tr_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     va_loss, va_acc = evaluate(model, val_loader, loss_fn)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m • \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% / val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, loader, loss_fn, optim, device)\u001b[39m\n\u001b[32m      9\u001b[39m     loss.backward()\n\u001b[32m     10\u001b[39m     optim.step()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     tot_loss    += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * x.size(\u001b[32m0\u001b[39m)\n\u001b[32m     12\u001b[39m     tot_correct += (logits.argmax(\u001b[32m1\u001b[39m) == y).sum().item()\n\u001b[32m     13\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pbounds = {\"lr\": (1e-4, 3e-3), \"dropout\": (0.0, 0.4), \"weight_decay\": (1e-6, 1e-3)}\n",
    "bo = BayesianOptimization(\n",
    "    f=lambda lr, dropout, weight_decay: train_eval_cnn(lr, dropout, weight_decay),\n",
    "    pbounds=pbounds, random_state=42, verbose=2\n",
    ")\n",
    "bo.maximize(init_points=5, n_iter=25)\n",
    "print(\"best →\", bo.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "test_loss, test_acc = evaluate(model, test_loader, loss_fn, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. deps ──────────────────────────────────────────────────────────────\n",
    "# pip install bayesian-optimization if you haven’t already\n",
    "from bayes_opt import BayesianOptimization\n",
    "import torch, time, json\n",
    "from pathlib import Path\n",
    "from classes.BSplineActivation import BSplineActivation\n",
    "# ── 1. make KANCNN fully hyper‑param friendly ────────────────────────────\n",
    "class KANCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Same backbone as before but every spline & layer size is configurable,\n",
    "    so BayesOpt can mess with them.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(3, *IMAGE_SIZE),\n",
    "                 conv_channels=(64, 128),\n",
    "                 kan_1=512,\n",
    "                 kan_2=256,\n",
    "                 kan_3=200,\n",
    "                 spline_cp=7,\n",
    "                 spline_deg=2,\n",
    "                 range_min=-3.0,\n",
    "                 range_max=50.0):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2 = conv_channels\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(C_in, c1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c2), nn.ReLU(inplace=True), nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flat = self.features(torch.zeros(1, *input_shape)).flatten(1).size(1)\n",
    "\n",
    "        self.kan1      = nn.Linear(flat, kan_1)\n",
    "        self.kan1_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan2      = nn.Linear(kan_1, kan_2)\n",
    "        self.kan2_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan3      = nn.Linear(kan_2, kan_3)\n",
    "        self.kan3_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.features(x), 1)\n",
    "        x = self.kan1(x)\n",
    "        x = self.kan1_act(x)\n",
    "        x = self.kan2(x)\n",
    "        x = self.kan2_act(x)\n",
    "        x = self.kan3(x)\n",
    "        x = self.kan3_act(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  training / eval helpers  ---------------------------\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Train {epoch:02d}\", leave=False):\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item()  * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item() * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "def run_training(model:nn.Module, name:str,\n",
    "                 train_loader:DataLoader, val_loader:DataLoader) -> Dict[str, List[float]]:\n",
    "    \"\"\"Full training loop for one model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        tic = time.time()\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "        val_metrics   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch:2d}/{N_EPOCHS} • \"\n",
    "              f\"train acc {train_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"val acc {val_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"Δt {time.time()-tic:4.1f}s\")\n",
    "\n",
    "    torch.save(model.state_dict(), RUN_DIR/f\"{name}.pt\")\n",
    "    with open(RUN_DIR/f\"{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df:pd.DataFrame):\n",
    "    \"\"\"Plot accuracy + loss curves for both models.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_acc\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_acc\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_acc\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_acc\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Accuracy vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"accuracy_curves.png\", dpi=200)\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_loss\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_loss\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_loss\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_loss\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Cross‑entropy loss\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Loss vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"loss_curves.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Final test evaluation\n",
    "    \n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "baseline = BaselineCNN()\n",
    "kan      = KANCNN()\n",
    "\n",
    "print(f'KAN parameters: {count_parameters(kan)}')\n",
    "print(f'Baseline parameters: {count_parameters(baseline)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"📚 Training baseline CNN …\")\n",
    "#hist_base, baseline_model = run_training(baseline, \"baseline\", train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n🌀 Training KAN‑CNN …\")\n",
    "#hist_kan  = run_training(kan,      \"KAN\",      train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#test_base = evaluate(baseline, test_loader, criterion)\n",
    "test_kan  = evaluate(kan,      test_loader, criterion)\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Test accuracy: \"\n",
    "        #f\"Baseline {test_base['acc']*100:5.2f}% | \"\n",
    "        f\"KAN {test_kan['acc']*100:5.2f}%\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    # ints → we pass floats in but will round later\n",
    "    \"epochs\":             (35, 55),\n",
    "    \"kan_1\":          (64, 512),   # width of first KAN layer\n",
    "    \"kan_2\":          (128, 512),   # second KAN layer\n",
    "    \"kan_3\":            (200, 200),\n",
    "    \"spline_cp\":          (6, 10),      # control points\n",
    "    \"spline_deg\":         (2, 5),      # deg ≤ cp‑1 guard enforced later\n",
    "    \"range_min\":          (-5.0, -0.5),\n",
    "    \"range_max\":          (5.0, 70.0),\n",
    "    \"lr\":                 (1e-4, 5e-3)\n",
    "}\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_kan(model, train_loader, val_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tic = time.time()\n",
    "        # ─ train ─\n",
    "        model.train()\n",
    "        loss_sum = acc_sum = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            acc = (logits.argmax(1) == y).float().mean().item()\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            acc_sum  += acc * x.size(0)\n",
    "\n",
    "        # ─ eval ─\n",
    "        model.eval()\n",
    "        loss_sum_val = acc_sum_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                logits = model(x)\n",
    "                loss   = criterion(logits, y)\n",
    "                acc    = (logits.argmax(1) == y).float().mean().item()\n",
    "                loss_sum_val += loss.item() * x.size(0)\n",
    "                acc_sum_val  += acc * x.size(0)\n",
    "\n",
    "        train_loss = loss_sum     / len(train_loader.dataset)\n",
    "        train_acc  = acc_sum      / len(train_loader.dataset)\n",
    "        val_loss   = loss_sum_val / len(val_loader.dataset)\n",
    "        val_acc    = acc_sum_val  / len(val_loader.dataset)\n",
    "        elapsed    = time.time() - tic\n",
    "\n",
    "        # ← print exactly like you had it\n",
    "        print(f\"Epoch [{ep}/{epochs}], \"\n",
    "              f\"Loss: {train_loss:.4f}, \"\n",
    "              f\"Test Acc: {val_acc*100:5.2f}%, \"\n",
    "              f\"Time: {elapsed:5.2f} seconds\")\n",
    "\n",
    "        best_val = max(best_val, val_acc)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "\n",
    "def optimize_kan(epochs,\n",
    "                 kan_inner,\n",
    "                 kan_outer,\n",
    "                 spline_cp,\n",
    "                 spline_deg,\n",
    "                 range_min,\n",
    "                 range_max,\n",
    "                 lr):\n",
    "\n",
    "    # ─ cast + sanity ─\n",
    "    epochs      = int(round(epochs))\n",
    "    kan_inner   = int(round(kan_inner))\n",
    "    kan_outer   = int(round(kan_outer))\n",
    "    spline_cp   = int(round(spline_cp))\n",
    "    spline_deg  = int(round(spline_deg))\n",
    "\n",
    "    # keep B‑spline well‑formed\n",
    "    spline_deg  = max(2, min(spline_deg, spline_cp - 1))\n",
    "    lr          = float(lr)\n",
    "\n",
    "    model = KANCNN(\n",
    "        kan_inner=kan_inner,\n",
    "        kan_outer=kan_outer,\n",
    "        spline_cp=spline_cp,\n",
    "        spline_deg=spline_deg,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max\n",
    "    )\n",
    "\n",
    "    val_acc = train_kan(model, train_loader, val_loader, epochs, lr)\n",
    "\n",
    "    # BayesOpt maximizes the returned value\n",
    "    return val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_kan,\n",
    "    pbounds=pbounds,\n",
    "    random_state=38,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 8 random warm‑ups + 10 guided iterations\n",
    "optimizer.maximize(init_points=6, n_iter=25)\n",
    "\n",
    "print(\"🚀 best combo so far →\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
