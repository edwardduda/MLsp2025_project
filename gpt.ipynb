{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, Union, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip, ToImage,\n",
    "    ToDtype, Normalize, Lambda                    # ⬅ new\n",
    ")\n",
    "from torchvision.utils import make_grid\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "import torch, os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip,\n",
    "    ToImage, ToDtype, Normalize\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# -----------------------  constants  ---------------------------------\n",
    "SEED               = 42\n",
    "N_EPOCHS           = 40\n",
    "BATCH_SIZE         = 128\n",
    "IMAGE_SIZE         = (64, 64)            # Tiny‑ImageNet native size\n",
    "LEARNING_RATE      = 0.005335\n",
    "WEIGHT_DECAY       = 1e-5\n",
    "MEAN        = [0.485, 0.456, 0.406]   # ImageNet stats\n",
    "STD         = [0.229, 0.224, 0.225]\n",
    "RUN_DIR            = Path(\"runs/tinyimagenet_cnn_vs_kan\").resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"➡️  Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    image_size: Tuple[int, int] = IMAGE_SIZE\n",
    "):\n",
    "    # ── 1. load and split ──────────────────────────────────────────────\n",
    "    ds       = load_dataset(\"zh-plus/tiny-imagenet\", split=None)      # train / valid\n",
    "    split    = ds[\"train\"].train_test_split(\n",
    "        test_size=0.10, seed=SEED, stratify_by_column=\"label\"\n",
    "    )\n",
    "    train_ds = split[\"train\"]           # 90 % → training\n",
    "    val_ds   = split[\"test\"]            # 10 % → internal val\n",
    "    test_ds  = ds[\"valid\"]              # official Tiny‑IN val → test set\n",
    "\n",
    "    # ── 2. define transforms ───────────────────────────────────────────\n",
    "    train_tfms = Compose([\n",
    "        ToImage(),                              # PIL → tensor\n",
    "        Resize(image_size, antialias=True),\n",
    "        RandomHorizontalFlip(p=0.5),\n",
    "        ToDtype(torch.float32, scale=True),     # convert + /255\n",
    "        Normalize(MEAN, STD),\n",
    "    ])\n",
    "    eval_tfms = Compose([\n",
    "        ToImage(),\n",
    "        Resize(image_size, antialias=True),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(MEAN, STD),\n",
    "    ])\n",
    "\n",
    "    # wrap for HF set_transform (expects dict in, dict out)\n",
    "    train_ds.set_transform(lambda ex: {\n",
    "        \"image\": train_tfms(ex[\"image\"]),\n",
    "        \"label\": ex[\"label\"],\n",
    "    })\n",
    "    for split_ds in (val_ds, test_ds):\n",
    "        split_ds.set_transform(lambda ex: {\n",
    "            \"image\": eval_tfms(ex[\"image\"]),\n",
    "            \"label\": ex[\"label\"],\n",
    "        })\n",
    "\n",
    "    # ── 3. torch DataLoaders ───────────────────────────────────────────\n",
    "    mk_loader = partial(\n",
    "        DataLoader,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    train_loader = mk_loader(train_ds, shuffle=True,  drop_last=True)\n",
    "    val_loader   = mk_loader(val_ds,   shuffle=False, drop_last=False)\n",
    "    test_loader  = mk_loader(test_ds,  shuffle=False, drop_last=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  models  --------------------------------------------\n",
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"A lightweight CNN with two conv blocks + MLP head.\"\"\"\n",
    "    def __init__(self, input_shape=(3, *IMAGE_SIZE),   # ⬅ 3‑channel\n",
    "                 conv_channels=(64, 128, 256)):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2, c3 = conv_channels\n",
    "        dropout = 0.1\n",
    "        self.features = nn.Sequential(\n",
    "        # Block 1\n",
    "        nn.Conv2d(C_in, c1, 3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.SiLU(inplace=True),\n",
    "        \n",
    "        # Block 2\n",
    "        nn.Conv2d(c1, c2, 3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.MaxPool2d(2),  # Now 32x32\n",
    "        \n",
    "        # Block 3\n",
    "        nn.Conv2d(c2, c3, 3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            flat_feats = self.features(torch.zeros(1, *input_shape)).view(1, -1).size(1)\n",
    "                \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(flat_feats, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 200)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. deps ──────────────────────────────────────────────────────────────\n",
    "# pip install bayesian-optimization if you haven’t already\n",
    "from bayes_opt import BayesianOptimization\n",
    "import torch, time, json\n",
    "from pathlib import Path\n",
    "from classes.BSplineActivation import BSplineActivation\n",
    "# ── 1. make KANCNN fully hyper‑param friendly ────────────────────────────\n",
    "class KANCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    7×7 stem + four two‑conv blocks (64‑128‑256‑512) → GAP → 3‑layer KAN head.\n",
    "    Every width / spline knob is exposed for BayesianOptimization.\n",
    "    \"\"\"\n",
    "    #| 21        | 0.2011    | 45.63     | 356.5     | 289.7     | 200.0     | 0.004335  | 42.08     | -1.016    | 7.936     | 3.244     |\n",
    "    #|   iter    |  target   |  epochs   |   kan_1   |   kan_2   |   kan_3   |    lr     | range_max | range_min | spline_cp | spline... |\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(3, *IMAGE_SIZE),\n",
    "                 stem_out=64,          # first 7×7 conv channels\n",
    "                 ch2=128, ch3=256, ch4=512,   # block widths\n",
    "                 kan_1=384, kan_2=256, kan_3=200,\n",
    "                 spline_cp=8, spline_deg=2,\n",
    "                 range_min=-3.0, range_max=70.0):\n",
    "        super().__init__()\n",
    "        C_in, *_ = input_shape\n",
    "\n",
    "        # ── 1. Convolutional backbone ──────────────────────────────────\n",
    "        self.features = nn.Sequential(\n",
    "            # Stem: 64×64 → 32×32\n",
    "            nn.Conv2d(C_in, stem_out, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(stem_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 32×32 → 16×16\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Block 1 (64 channels, no down‑sample) 16×16 → 16×16\n",
    "            nn.Conv2d(stem_out, stem_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(stem_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(stem_out, stem_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(stem_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Block 2 (128 channels, stride‑2) 16×16 → 8×8\n",
    "            nn.Conv2d(stem_out, ch2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch2, ch2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Block 3 (256 channels, stride‑2) 8×8 → 4×4\n",
    "            nn.Conv2d(ch2, ch3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch3, ch3, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Block 4 (512 channels, stride‑2) 4×4 → 2×2\n",
    "            nn.Conv2d(ch3, ch4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch4, ch4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Global average pool 2×2 → 1×1\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        flat_dim = ch4  # after GAP you always have 512 (or ch4)\n",
    "\n",
    "        # ── 2. KAN head with B‑spline activations ───────────────────────\n",
    "        self.kan1 = nn.Linear(flat_dim, kan_1)\n",
    "        self.kan1_act = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan2 = nn.Linear(kan_1, kan_2)\n",
    "        self.kan2_act = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan3 = nn.Linear(kan_2, kan_3)\n",
    "        self.kan3_act = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "\n",
    "    # ── 3. Forward pass ────────────────────────────────────────────────\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)          # B × 512 × 1 × 1\n",
    "        x = torch.flatten(x, 1)       # B × 512\n",
    "        x = self.kan1_act(self.kan1(x))\n",
    "        x = self.kan2_act(self.kan2(x))\n",
    "        x = self.kan3_act(self.kan3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  training / eval helpers  ---------------------------\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Train {epoch:02d}\", leave=False):\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item()  * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item() * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "def run_training(model:nn.Module, name:str,\n",
    "                 train_loader:DataLoader, val_loader:DataLoader) -> Dict[str, List[float]]:\n",
    "    \"\"\"Full training loop for one model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        tic = time.time()\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "        val_metrics   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch:2d}/{N_EPOCHS} • \"\n",
    "              f\"train acc {train_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"val acc {val_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"Δt {time.time()-tic:4.1f}s\")\n",
    "\n",
    "    torch.save(model.state_dict(), RUN_DIR/f\"{name}.pt\")\n",
    "    with open(RUN_DIR/f\"{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df:pd.DataFrame):\n",
    "    \"\"\"Plot accuracy + loss curves for both models.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_acc\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_acc\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_acc\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_acc\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Accuracy vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"accuracy_curves.png\", dpi=200)\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_loss\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_loss\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_loss\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_loss\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Cross‑entropy loss\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Loss vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"loss_curves.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAN parameters: 5078944\n",
      "Baseline parameters: 13661840\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Final test evaluation\n",
    "    \n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "baseline = BaselineCNN()\n",
    "kan      = KANCNN()\n",
    "\n",
    "print(f'KAN parameters: {count_parameters(kan)}')\n",
    "print(f'Baseline parameters: {count_parameters(baseline)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Training baseline CNN …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (128x200 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📚 Training baseline CNN …\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m hist_base, baseline_model = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbaseline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model, name, train_loader, val_loader)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, N_EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m     50\u001b[39m     tic = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     train_metrics = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     val_metrics   = evaluate(model, val_loader, criterion)\n\u001b[32m     54\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_metrics[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     10\u001b[39m y = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m loss = criterion(logits, y)\n\u001b[32m     15\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mBaselineCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     44\u001b[39m x = \u001b[38;5;28mself\u001b[39m.features(x)\n\u001b[32m     45\u001b[39m x = \u001b[38;5;28mself\u001b[39m.flatten(x)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: linear(): input and weight.T shapes cannot be multiplied (128x200 and 512x256)"
     ]
    }
   ],
   "source": [
    "print(\"📚 Training baseline CNN …\")\n",
    "hist_base, baseline_model = run_training(baseline, \"baseline\", train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌀 Training KAN‑CNN …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🌀 Training KAN‑CNN …\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m hist_kan  = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mKAN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model, name, train_loader, val_loader)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, N_EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m     50\u001b[39m     tic = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     train_metrics = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     val_metrics   = evaluate(model, val_loader, criterion)\n\u001b[32m     54\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_metrics[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     10\u001b[39m y = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m loss = criterion(logits, y)\n\u001b[32m     15\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mKANCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# B × 512 × 1 × 1\u001b[39;00m\n\u001b[32m     99\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)       \u001b[38;5;66;03m# B × 512\u001b[39;00m\n\u001b[32m    100\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.kan1_act(\u001b[38;5;28mself\u001b[39m.kan1(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n🌀 Training KAN‑CNN …\")\n",
    "hist_kan  = run_training(kan,      \"KAN\",      train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test accuracy: KAN  0.65%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#test_base = evaluate(baseline, test_loader, criterion)\n",
    "test_kan  = evaluate(kan,      test_loader, criterion)\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Test accuracy: \"\n",
    "        #f\"Baseline {test_base['acc']*100:5.2f}% | \"\n",
    "        f\"KAN {test_kan['acc']*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    # ints → we pass floats in but will round later\n",
    "    \"epochs\":             (20, 50),\n",
    "    \"kan_1\":          (100, 512),   # width of first KAN layer\n",
    "    \"kan_2\":          (100, 512),   # second KAN layer\n",
    "    \"kan_3\":            (200, 200),\n",
    "    \"spline_cp\":          (5, 10),      # control points\n",
    "    \"spline_deg\":         (2, 4),      # deg ≤ cp‑1 guard enforced later\n",
    "    \"range_min\":          (-2.0, -0.5),\n",
    "    \"range_max\":          (40.0, 80.0),\n",
    "    \"lr\":                 (1e-4, 1e-2)\n",
    "}\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_kan(model, train_loader, val_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tic = time.time()\n",
    "        # ─ train ─\n",
    "        model.train()\n",
    "        loss_sum = acc_sum = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            acc = (logits.argmax(1) == y).float().mean().item()\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            acc_sum  += acc * x.size(0)\n",
    "\n",
    "        # ─ eval ─\n",
    "        model.eval()\n",
    "        loss_sum_val = acc_sum_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                logits = model(x)\n",
    "                loss   = criterion(logits, y)\n",
    "                acc    = (logits.argmax(1) == y).float().mean().item()\n",
    "                loss_sum_val += loss.item() * x.size(0)\n",
    "                acc_sum_val  += acc * x.size(0)\n",
    "\n",
    "        train_loss = loss_sum     / len(train_loader.dataset)\n",
    "        train_acc  = acc_sum      / len(train_loader.dataset)\n",
    "        val_loss   = loss_sum_val / len(val_loader.dataset)\n",
    "        val_acc    = acc_sum_val  / len(val_loader.dataset)\n",
    "        elapsed    = time.time() - tic\n",
    "\n",
    "        # ← print exactly like you had it\n",
    "        print(f\"Epoch [{ep}/{epochs}], \"\n",
    "              f\"Loss: {train_loss:.4f}, \"\n",
    "              f\"Test Acc: {val_acc*100:5.2f}%, \"\n",
    "              f\"Time: {elapsed:5.2f} seconds\")\n",
    "\n",
    "        best_val = max(best_val, val_acc)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "\n",
    "def optimize_kan(epochs,\n",
    "                 kan_1,\n",
    "                 kan_2,\n",
    "                 kan_3,\n",
    "                 spline_cp,\n",
    "                 spline_deg,\n",
    "                 range_min,\n",
    "                 range_max,\n",
    "                 lr):\n",
    "\n",
    "    # ─ cast + sanity ─\n",
    "    epochs      = int(round(epochs))\n",
    "    kan_1   = int(round(kan_1))\n",
    "    kan_2   = int(round(kan_2))\n",
    "    kan_3   = int(round(kan_3))\n",
    "    spline_cp   = int(round(spline_cp))\n",
    "    spline_deg  = int(round(spline_deg))\n",
    "\n",
    "    # keep B‑spline well‑formed\n",
    "    spline_deg  = max(2, min(spline_deg, spline_cp - 1))\n",
    "    lr          = float(lr)\n",
    "\n",
    "    model = KANCNN(\n",
    "        kan_1=kan_1,\n",
    "        kan_2=kan_2,\n",
    "        kan_3=kan_3,\n",
    "        spline_cp=spline_cp,\n",
    "        spline_deg=spline_deg,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max\n",
    "    )\n",
    "\n",
    "    val_acc = train_kan(model, train_loader, val_loader, epochs, lr)\n",
    "\n",
    "    # BayesOpt maximizes the returned value\n",
    "    return val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  epochs   |   kan_1   |   kan_2   |   kan_3   |    lr     | range_max | range_min | spline_cp | spline... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1/32], Loss: 5.1789, Test Acc:  1.44%, Time: 31.75 seconds\n",
      "Epoch [2/32], Loss: 5.0018, Test Acc:  2.28%, Time: 31.78 seconds\n",
      "Epoch [3/32], Loss: 4.7337, Test Acc:  4.54%, Time: 31.69 seconds\n",
      "Epoch [4/32], Loss: 4.3921, Test Acc:  6.99%, Time: 31.70 seconds\n",
      "Epoch [5/32], Loss: 4.1791, Test Acc:  8.48%, Time: 31.78 seconds\n",
      "Epoch [6/32], Loss: 3.9966, Test Acc: 12.26%, Time: 31.60 seconds\n",
      "Epoch [7/32], Loss: 3.8374, Test Acc: 12.88%, Time: 31.71 seconds\n",
      "Epoch [8/32], Loss: 3.7149, Test Acc: 15.55%, Time: 31.75 seconds\n",
      "Epoch [9/32], Loss: 3.6029, Test Acc: 17.54%, Time: 31.68 seconds\n",
      "Epoch [10/32], Loss: 3.5037, Test Acc: 18.75%, Time: 31.69 seconds\n",
      "Epoch [11/32], Loss: 3.3966, Test Acc: 20.42%, Time: 31.77 seconds\n",
      "Epoch [12/32], Loss: 3.2964, Test Acc: 21.25%, Time: 31.64 seconds\n",
      "Epoch [13/32], Loss: 3.2074, Test Acc: 22.37%, Time: 31.67 seconds\n",
      "Epoch [14/32], Loss: 3.1319, Test Acc: 23.24%, Time: 31.82 seconds\n",
      "Epoch [15/32], Loss: 3.0482, Test Acc: 23.84%, Time: 31.61 seconds\n",
      "Epoch [16/32], Loss: 2.9810, Test Acc: 24.96%, Time: 31.69 seconds\n",
      "Epoch [17/32], Loss: 2.9035, Test Acc: 24.63%, Time: 31.82 seconds\n",
      "Epoch [18/32], Loss: 2.8370, Test Acc: 26.45%, Time: 31.70 seconds\n",
      "Epoch [19/32], Loss: 2.7657, Test Acc: 25.66%, Time: 31.66 seconds\n",
      "Epoch [20/32], Loss: 2.7020, Test Acc: 26.44%, Time: 31.84 seconds\n",
      "Epoch [21/32], Loss: 2.6360, Test Acc: 27.85%, Time: 31.67 seconds\n",
      "Epoch [22/32], Loss: 2.5689, Test Acc: 27.91%, Time: 31.59 seconds\n",
      "Epoch [23/32], Loss: 2.5059, Test Acc: 27.45%, Time: 32.09 seconds\n",
      "Epoch [24/32], Loss: 2.4369, Test Acc: 28.74%, Time: 31.61 seconds\n",
      "Epoch [25/32], Loss: 2.3798, Test Acc: 27.97%, Time: 31.73 seconds\n",
      "Epoch [26/32], Loss: 2.3158, Test Acc: 28.36%, Time: 31.75 seconds\n",
      "Epoch [27/32], Loss: 2.2550, Test Acc: 28.17%, Time: 31.56 seconds\n",
      "Epoch [28/32], Loss: 2.1938, Test Acc: 28.49%, Time: 31.56 seconds\n",
      "Epoch [29/32], Loss: 2.1237, Test Acc: 28.50%, Time: 31.61 seconds\n",
      "Epoch [30/32], Loss: 2.0677, Test Acc: 28.35%, Time: 31.63 seconds\n",
      "Epoch [31/32], Loss: 2.0058, Test Acc: 27.93%, Time: 31.48 seconds\n",
      "Epoch [32/32], Loss: 1.9452, Test Acc: 28.04%, Time: 31.57 seconds\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.2874   \u001b[39m | \u001b[39m31.54    \u001b[39m | \u001b[39m454.2    \u001b[39m | \u001b[39m489.0    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.006373 \u001b[39m | \u001b[39m64.24    \u001b[39m | \u001b[39m-1.7     \u001b[39m | \u001b[39m6.937    \u001b[39m | \u001b[39m2.518    \u001b[39m |\n",
      "Epoch [1/22], Loss: 5.2134, Test Acc:  1.17%, Time: 31.58 seconds\n",
      "Epoch [2/22], Loss: 5.0380, Test Acc:  2.08%, Time: 31.45 seconds\n",
      "Epoch [3/22], Loss: 4.8593, Test Acc:  3.54%, Time: 31.36 seconds\n",
      "Epoch [4/22], Loss: 4.5974, Test Acc:  6.33%, Time: 31.29 seconds\n",
      "Epoch [5/22], Loss: 4.4136, Test Acc:  7.59%, Time: 31.44 seconds\n",
      "Epoch [6/22], Loss: 4.2176, Test Acc: 10.01%, Time: 31.32 seconds\n",
      "Epoch [7/22], Loss: 4.0311, Test Acc: 11.64%, Time: 31.28 seconds\n",
      "Epoch [8/22], Loss: 3.9025, Test Acc: 13.97%, Time: 31.41 seconds\n",
      "Epoch [9/22], Loss: 3.7819, Test Acc: 14.77%, Time: 31.34 seconds\n",
      "Epoch [10/22], Loss: 3.6799, Test Acc: 16.30%, Time: 31.28 seconds\n",
      "Epoch [11/22], Loss: 3.5937, Test Acc: 16.34%, Time: 31.46 seconds\n",
      "Epoch [12/22], Loss: 3.5066, Test Acc: 18.12%, Time: 31.29 seconds\n",
      "Epoch [13/22], Loss: 3.4257, Test Acc: 19.60%, Time: 31.41 seconds\n",
      "Epoch [14/22], Loss: 3.3565, Test Acc: 19.52%, Time: 31.71 seconds\n",
      "Epoch [15/22], Loss: 3.2803, Test Acc: 22.96%, Time: 31.56 seconds\n",
      "Epoch [16/22], Loss: 3.2185, Test Acc: 22.21%, Time: 31.55 seconds\n",
      "Epoch [17/22], Loss: 3.1539, Test Acc: 23.37%, Time: 31.52 seconds\n",
      "Epoch [18/22], Loss: 3.0960, Test Acc: 23.92%, Time: 31.37 seconds\n",
      "Epoch [19/22], Loss: 3.0394, Test Acc: 24.36%, Time: 31.31 seconds\n",
      "Epoch [20/22], Loss: 2.9828, Test Acc: 23.32%, Time: 31.61 seconds\n",
      "Epoch [21/22], Loss: 2.9258, Test Acc: 25.31%, Time: 31.31 seconds\n",
      "Epoch [22/22], Loss: 2.8720, Test Acc: 26.52%, Time: 31.33 seconds\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.2652   \u001b[39m | \u001b[39m22.24    \u001b[39m | \u001b[39m215.8    \u001b[39m | \u001b[39m280.6    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.008698 \u001b[39m | \u001b[39m61.19    \u001b[39m | \u001b[39m-1.157   \u001b[39m | \u001b[39m7.355    \u001b[39m | \u001b[39m3.378    \u001b[39m |\n",
      "Epoch [1/31], Loss: 5.1495, Test Acc:  1.35%, Time: 31.95 seconds\n",
      "Epoch [2/31], Loss: 4.9176, Test Acc:  3.77%, Time: 31.47 seconds\n",
      "Epoch [3/31], Loss: 4.5484, Test Acc:  6.40%, Time: 31.42 seconds\n",
      "Epoch [4/31], Loss: 4.2736, Test Acc:  8.12%, Time: 31.52 seconds\n",
      "Epoch [5/31], Loss: 4.0791, Test Acc: 11.60%, Time: 31.43 seconds\n",
      "Epoch [6/31], Loss: 3.8910, Test Acc: 14.16%, Time: 31.48 seconds\n",
      "Epoch [7/31], Loss: 3.7309, Test Acc: 15.59%, Time: 31.57 seconds\n",
      "Epoch [8/31], Loss: 3.5929, Test Acc: 17.72%, Time: 31.40 seconds\n",
      "Epoch [9/31], Loss: 3.4749, Test Acc: 18.87%, Time: 31.46 seconds\n",
      "Epoch [10/31], Loss: 3.3697, Test Acc: 20.40%, Time: 31.53 seconds\n",
      "Epoch [11/31], Loss: 3.2725, Test Acc: 21.23%, Time: 31.48 seconds\n",
      "Epoch [12/31], Loss: 3.1859, Test Acc: 22.81%, Time: 31.45 seconds\n",
      "Epoch [13/31], Loss: 3.1049, Test Acc: 23.26%, Time: 31.75 seconds\n",
      "Epoch [14/31], Loss: 3.0276, Test Acc: 24.98%, Time: 31.45 seconds\n",
      "Epoch [15/31], Loss: 2.9550, Test Acc: 25.68%, Time: 31.42 seconds\n",
      "Epoch [16/31], Loss: 2.8769, Test Acc: 25.55%, Time: 31.50 seconds\n",
      "Epoch [17/31], Loss: 2.7953, Test Acc: 26.32%, Time: 31.45 seconds\n",
      "Epoch [18/31], Loss: 2.7245, Test Acc: 26.67%, Time: 31.44 seconds\n",
      "Epoch [19/31], Loss: 2.6449, Test Acc: 27.56%, Time: 31.60 seconds\n",
      "Epoch [20/31], Loss: 2.5684, Test Acc: 28.46%, Time: 31.41 seconds\n",
      "Epoch [21/31], Loss: 2.4886, Test Acc: 28.58%, Time: 31.41 seconds\n",
      "Epoch [22/31], Loss: 2.4137, Test Acc: 29.23%, Time: 31.50 seconds\n",
      "Epoch [23/31], Loss: 2.3363, Test Acc: 28.63%, Time: 31.46 seconds\n",
      "Epoch [24/31], Loss: 2.2621, Test Acc: 29.47%, Time: 31.42 seconds\n",
      "Epoch [25/31], Loss: 2.1760, Test Acc: 29.17%, Time: 31.60 seconds\n",
      "Epoch [26/31], Loss: 2.1069, Test Acc: 29.76%, Time: 31.42 seconds\n",
      "Epoch [27/31], Loss: 2.0199, Test Acc: 29.30%, Time: 31.43 seconds\n",
      "Epoch [28/31], Loss: 1.9432, Test Acc: 29.96%, Time: 31.47 seconds\n",
      "Epoch [29/31], Loss: 1.8707, Test Acc: 29.74%, Time: 31.55 seconds\n",
      "Epoch [30/31], Loss: 1.7870, Test Acc: 29.72%, Time: 31.43 seconds\n",
      "Epoch [31/31], Loss: 1.7125, Test Acc: 29.27%, Time: 31.41 seconds\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.2996   \u001b[39m | \u001b[35m31.03    \u001b[39m | \u001b[35m456.4    \u001b[39m | \u001b[35m485.6    \u001b[39m | \u001b[35m200.0    \u001b[39m | \u001b[35m0.003157 \u001b[39m | \u001b[35m65.3     \u001b[39m | \u001b[35m-1.524   \u001b[39m | \u001b[35m9.993    \u001b[39m | \u001b[35m3.191    \u001b[39m |\n",
      "Epoch [1/33], Loss: 5.1781, Test Acc:  1.44%, Time: 31.96 seconds\n",
      "Epoch [2/33], Loss: 5.0198, Test Acc:  2.31%, Time: 31.46 seconds\n",
      "Epoch [3/33], Loss: 4.8783, Test Acc:  3.18%, Time: 31.43 seconds\n",
      "Epoch [4/33], Loss: 4.7064, Test Acc:  4.77%, Time: 31.53 seconds\n",
      "Epoch [5/33], Loss: 4.4490, Test Acc:  6.73%, Time: 31.37 seconds\n",
      "Epoch [6/33], Loss: 4.1846, Test Acc: 10.80%, Time: 31.41 seconds\n",
      "Epoch [7/33], Loss: 4.0019, Test Acc: 11.32%, Time: 31.53 seconds\n",
      "Epoch [8/33], Loss: 3.8459, Test Acc: 13.90%, Time: 31.39 seconds\n",
      "Epoch [9/33], Loss: 3.7174, Test Acc: 15.31%, Time: 31.44 seconds\n",
      "Epoch [10/33], Loss: 3.6158, Test Acc: 16.75%, Time: 31.52 seconds\n",
      "Epoch [11/33], Loss: 3.5300, Test Acc: 17.73%, Time: 31.45 seconds\n",
      "Epoch [12/33], Loss: 3.4514, Test Acc: 17.84%, Time: 31.34 seconds\n",
      "Epoch [13/33], Loss: 3.3773, Test Acc: 20.33%, Time: 31.52 seconds\n",
      "Epoch [14/33], Loss: 3.3030, Test Acc: 20.99%, Time: 31.46 seconds\n",
      "Epoch [15/33], Loss: 3.2242, Test Acc: 21.62%, Time: 31.47 seconds\n",
      "Epoch [16/33], Loss: 3.1537, Test Acc: 22.26%, Time: 31.47 seconds\n",
      "Epoch [17/33], Loss: 3.0888, Test Acc: 23.02%, Time: 31.47 seconds\n",
      "Epoch [18/33], Loss: 3.0193, Test Acc: 24.08%, Time: 31.42 seconds\n",
      "Epoch [19/33], Loss: 2.9602, Test Acc: 23.95%, Time: 31.53 seconds\n",
      "Epoch [20/33], Loss: 2.8991, Test Acc: 23.31%, Time: 31.46 seconds\n",
      "Epoch [21/33], Loss: 2.8386, Test Acc: 24.76%, Time: 31.44 seconds\n",
      "Epoch [22/33], Loss: 2.7780, Test Acc: 24.64%, Time: 31.52 seconds\n",
      "Epoch [23/33], Loss: 2.7217, Test Acc: 25.57%, Time: 31.44 seconds\n",
      "Epoch [24/33], Loss: 2.6635, Test Acc: 25.73%, Time: 31.42 seconds\n",
      "Epoch [25/33], Loss: 2.6028, Test Acc: 25.46%, Time: 31.58 seconds\n",
      "Epoch [26/33], Loss: 2.5472, Test Acc: 24.87%, Time: 31.43 seconds\n",
      "Epoch [27/33], Loss: 2.4857, Test Acc: 25.46%, Time: 31.44 seconds\n",
      "Epoch [28/33], Loss: 2.4285, Test Acc: 26.09%, Time: 31.51 seconds\n",
      "Epoch [29/33], Loss: 2.3721, Test Acc: 27.23%, Time: 31.46 seconds\n",
      "Epoch [30/33], Loss: 2.3158, Test Acc: 27.27%, Time: 31.41 seconds\n",
      "Epoch [31/33], Loss: 2.2594, Test Acc: 27.58%, Time: 31.53 seconds\n",
      "Epoch [32/33], Loss: 2.1968, Test Acc: 26.06%, Time: 31.40 seconds\n",
      "Epoch [33/33], Loss: 2.1397, Test Acc: 26.92%, Time: 31.40 seconds\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.2758   \u001b[39m | \u001b[39m33.17    \u001b[39m | \u001b[39m461.4    \u001b[39m | \u001b[39m481.4    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.005353 \u001b[39m | \u001b[39m54.46    \u001b[39m | \u001b[39m-1.673   \u001b[39m | \u001b[39m8.261    \u001b[39m | \u001b[39m2.914    \u001b[39m |\n",
      "Epoch [1/38], Loss: 5.1514, Test Acc:  1.63%, Time: 31.87 seconds\n",
      "Epoch [2/38], Loss: 4.9868, Test Acc:  2.59%, Time: 31.53 seconds\n",
      "Epoch [3/38], Loss: 4.7038, Test Acc:  5.58%, Time: 31.35 seconds\n",
      "Epoch [4/38], Loss: 4.3671, Test Acc:  7.83%, Time: 31.38 seconds\n",
      "Epoch [5/38], Loss: 4.1774, Test Acc:  9.43%, Time: 31.45 seconds\n",
      "Epoch [6/38], Loss: 4.0215, Test Acc: 10.90%, Time: 31.37 seconds\n",
      "Epoch [7/38], Loss: 3.8442, Test Acc: 14.64%, Time: 31.40 seconds\n",
      "Epoch [8/38], Loss: 3.6854, Test Acc: 16.00%, Time: 31.48 seconds\n",
      "Epoch [9/38], Loss: 3.5601, Test Acc: 18.16%, Time: 31.27 seconds\n",
      "Epoch [10/38], Loss: 3.4506, Test Acc: 19.04%, Time: 31.32 seconds\n",
      "Epoch [11/38], Loss: 3.3559, Test Acc: 20.14%, Time: 31.46 seconds\n",
      "Epoch [12/38], Loss: 3.2628, Test Acc: 21.27%, Time: 31.38 seconds\n",
      "Epoch [13/38], Loss: 3.1812, Test Acc: 22.23%, Time: 31.34 seconds\n",
      "Epoch [14/38], Loss: 3.1065, Test Acc: 22.78%, Time: 31.47 seconds\n",
      "Epoch [15/38], Loss: 3.0336, Test Acc: 23.95%, Time: 31.39 seconds\n",
      "Epoch [16/38], Loss: 2.9628, Test Acc: 25.20%, Time: 31.34 seconds\n",
      "Epoch [17/38], Loss: 2.8970, Test Acc: 25.14%, Time: 31.43 seconds\n",
      "Epoch [18/38], Loss: 2.8357, Test Acc: 25.49%, Time: 31.37 seconds\n",
      "Epoch [19/38], Loss: 2.7690, Test Acc: 24.97%, Time: 31.32 seconds\n",
      "Epoch [20/38], Loss: 2.7102, Test Acc: 26.24%, Time: 31.44 seconds\n",
      "Epoch [21/38], Loss: 2.6451, Test Acc: 26.91%, Time: 31.33 seconds\n",
      "Epoch [22/38], Loss: 2.5929, Test Acc: 27.30%, Time: 31.38 seconds\n",
      "Epoch [23/38], Loss: 2.5344, Test Acc: 26.73%, Time: 31.44 seconds\n",
      "Epoch [24/38], Loss: 2.4777, Test Acc: 27.10%, Time: 31.37 seconds\n",
      "Epoch [25/38], Loss: 2.4205, Test Acc: 27.07%, Time: 31.33 seconds\n",
      "Epoch [26/38], Loss: 2.3577, Test Acc: 27.35%, Time: 31.43 seconds\n",
      "Epoch [27/38], Loss: 2.3005, Test Acc: 26.95%, Time: 31.33 seconds\n",
      "Epoch [28/38], Loss: 2.2421, Test Acc: 27.67%, Time: 31.42 seconds\n",
      "Epoch [29/38], Loss: 2.1803, Test Acc: 27.79%, Time: 31.41 seconds\n",
      "Epoch [30/38], Loss: 2.1286, Test Acc: 28.12%, Time: 31.33 seconds\n",
      "Epoch [31/38], Loss: 2.0596, Test Acc: 27.90%, Time: 31.25 seconds\n",
      "Epoch [32/38], Loss: 1.9996, Test Acc: 28.17%, Time: 31.48 seconds\n",
      "Epoch [33/38], Loss: 1.9465, Test Acc: 28.26%, Time: 31.34 seconds\n",
      "Epoch [34/38], Loss: 1.8862, Test Acc: 28.41%, Time: 31.39 seconds\n",
      "Epoch [35/38], Loss: 1.8376, Test Acc: 28.42%, Time: 31.47 seconds\n",
      "Epoch [36/38], Loss: 1.7596, Test Acc: 27.57%, Time: 31.38 seconds\n",
      "Epoch [37/38], Loss: 1.7057, Test Acc: 27.71%, Time: 31.39 seconds\n",
      "Epoch [38/38], Loss: 1.6533, Test Acc: 28.30%, Time: 31.43 seconds\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.2842   \u001b[39m | \u001b[39m38.02    \u001b[39m | \u001b[39m459.8    \u001b[39m | \u001b[39m483.8    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.00428  \u001b[39m | \u001b[39m65.89    \u001b[39m | \u001b[39m-0.8278  \u001b[39m | \u001b[39m6.486    \u001b[39m | \u001b[39m2.874    \u001b[39m |\n",
      "Epoch [1/32], Loss: 5.1532, Test Acc:  1.81%, Time: 33.11 seconds\n",
      "Epoch [2/32], Loss: 4.9429, Test Acc:  2.70%, Time: 32.55 seconds\n",
      "Epoch [3/32], Loss: 4.6243, Test Acc:  6.20%, Time: 32.60 seconds\n",
      "Epoch [4/32], Loss: 4.3632, Test Acc:  8.19%, Time: 32.52 seconds\n",
      "Epoch [5/32], Loss: 4.1851, Test Acc:  9.49%, Time: 32.73 seconds\n",
      "Epoch [6/32], Loss: 4.0086, Test Acc: 12.21%, Time: 32.54 seconds\n",
      "Epoch [7/32], Loss: 3.8492, Test Acc: 13.92%, Time: 32.64 seconds\n",
      "Epoch [8/32], Loss: 3.7122, Test Acc: 16.38%, Time: 32.93 seconds\n",
      "Epoch [9/32], Loss: 3.5878, Test Acc: 18.75%, Time: 32.80 seconds\n",
      "Epoch [10/32], Loss: 3.4795, Test Acc: 18.49%, Time: 32.81 seconds\n",
      "Epoch [11/32], Loss: 3.3734, Test Acc: 20.22%, Time: 32.93 seconds\n",
      "Epoch [12/32], Loss: 3.2834, Test Acc: 21.68%, Time: 32.84 seconds\n",
      "Epoch [13/32], Loss: 3.1996, Test Acc: 22.27%, Time: 32.72 seconds\n",
      "Epoch [14/32], Loss: 3.1204, Test Acc: 23.30%, Time: 32.79 seconds\n",
      "Epoch [15/32], Loss: 3.0315, Test Acc: 24.17%, Time: 32.68 seconds\n",
      "Epoch [16/32], Loss: 2.9578, Test Acc: 25.61%, Time: 32.69 seconds\n",
      "Epoch [17/32], Loss: 2.8842, Test Acc: 25.77%, Time: 32.77 seconds\n",
      "Epoch [18/32], Loss: 2.8071, Test Acc: 26.04%, Time: 32.69 seconds\n",
      "Epoch [19/32], Loss: 2.7330, Test Acc: 27.04%, Time: 32.53 seconds\n",
      "Epoch [20/32], Loss: 2.6527, Test Acc: 27.98%, Time: 32.53 seconds\n",
      "Epoch [21/32], Loss: 2.5822, Test Acc: 27.34%, Time: 32.63 seconds\n",
      "Epoch [22/32], Loss: 2.5117, Test Acc: 28.63%, Time: 32.43 seconds\n",
      "Epoch [23/32], Loss: 2.4315, Test Acc: 28.39%, Time: 32.50 seconds\n",
      "Epoch [24/32], Loss: 2.3554, Test Acc: 28.59%, Time: 32.41 seconds\n",
      "Epoch [25/32], Loss: 2.2731, Test Acc: 28.99%, Time: 32.34 seconds\n",
      "Epoch [26/32], Loss: 2.1914, Test Acc: 27.46%, Time: 32.52 seconds\n",
      "Epoch [27/32], Loss: 2.1156, Test Acc: 29.14%, Time: 32.41 seconds\n",
      "Epoch [28/32], Loss: 2.0322, Test Acc: 29.42%, Time: 32.34 seconds\n",
      "Epoch [29/32], Loss: 1.9478, Test Acc: 29.19%, Time: 32.40 seconds\n",
      "Epoch [30/32], Loss: 1.8711, Test Acc: 29.30%, Time: 32.26 seconds\n",
      "Epoch [31/32], Loss: 1.7881, Test Acc: 28.95%, Time: 32.27 seconds\n",
      "Epoch [32/32], Loss: 1.7047, Test Acc: 28.21%, Time: 32.47 seconds\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.2942   \u001b[39m | \u001b[39m32.34    \u001b[39m | \u001b[39m462.3    \u001b[39m | \u001b[39m488.4    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.005939 \u001b[39m | \u001b[39m62.89    \u001b[39m | \u001b[39m-1.637   \u001b[39m | \u001b[39m9.267    \u001b[39m | \u001b[39m3.922    \u001b[39m |\n",
      "Epoch [1/25], Loss: 5.2928, Test Acc:  0.50%, Time: 31.59 seconds\n",
      "Epoch [2/25], Loss: 5.2974, Test Acc:  0.48%, Time: 31.11 seconds\n",
      "Epoch [3/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.18 seconds\n",
      "Epoch [4/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.30 seconds\n",
      "Epoch [5/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.17 seconds\n",
      "Epoch [6/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.16 seconds\n",
      "Epoch [7/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.28 seconds\n",
      "Epoch [8/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.16 seconds\n",
      "Epoch [9/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.16 seconds\n",
      "Epoch [10/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.34 seconds\n",
      "Epoch [11/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.26 seconds\n",
      "Epoch [12/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.25 seconds\n",
      "Epoch [13/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.18 seconds\n",
      "Epoch [14/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.33 seconds\n",
      "Epoch [15/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.25 seconds\n",
      "Epoch [16/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.22 seconds\n",
      "Epoch [17/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.30 seconds\n",
      "Epoch [18/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.22 seconds\n",
      "Epoch [19/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.14 seconds\n",
      "Epoch [20/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.33 seconds\n",
      "Epoch [21/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.20 seconds\n",
      "Epoch [22/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.19 seconds\n",
      "Epoch [23/25], Loss: 5.2975, Test Acc:  0.50%, Time: 31.29 seconds\n",
      "Epoch [24/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.20 seconds\n",
      "Epoch [25/25], Loss: 5.2974, Test Acc:  0.50%, Time: 31.22 seconds\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.005    \u001b[39m | \u001b[39m25.29    \u001b[39m | \u001b[39m458.3    \u001b[39m | \u001b[39m488.8    \u001b[39m | \u001b[39m200.0    \u001b[39m | \u001b[39m0.007769 \u001b[39m | \u001b[39m70.57    \u001b[39m | \u001b[39m-1.731   \u001b[39m | \u001b[39m8.134    \u001b[39m | \u001b[39m2.753    \u001b[39m |\n",
      "Epoch [1/30], Loss: 5.2284, Test Acc:  1.16%, Time: 31.62 seconds\n",
      "Epoch [2/30], Loss: 5.0910, Test Acc:  1.88%, Time: 31.21 seconds\n",
      "Epoch [3/30], Loss: 5.0037, Test Acc:  2.05%, Time: 31.18 seconds\n",
      "Epoch [4/30], Loss: 4.8985, Test Acc:  2.76%, Time: 31.22 seconds\n",
      "Epoch [5/30], Loss: 4.8138, Test Acc:  3.35%, Time: 31.19 seconds\n",
      "Epoch [6/30], Loss: 4.7253, Test Acc:  3.92%, Time: 31.18 seconds\n",
      "Epoch [7/30], Loss: 4.6352, Test Acc:  4.08%, Time: 31.23 seconds\n",
      "Epoch [8/30], Loss: 4.5574, Test Acc:  4.74%, Time: 31.19 seconds\n",
      "Epoch [9/30], Loss: 4.4933, Test Acc:  5.05%, Time: 31.15 seconds\n",
      "Epoch [10/30], Loss: 4.3761, Test Acc:  6.73%, Time: 31.32 seconds\n",
      "Epoch [11/30], Loss: 4.2852, Test Acc:  7.49%, Time: 31.18 seconds\n",
      "Epoch [12/30], Loss: 4.2143, Test Acc:  8.62%, Time: 31.18 seconds\n",
      "Epoch [13/30], Loss: 4.1417, Test Acc:  9.04%, Time: 31.24 seconds\n",
      "Epoch [14/30], Loss: 4.0481, Test Acc: 10.72%, Time: 31.21 seconds\n",
      "Epoch [15/30], Loss: 3.9502, Test Acc: 10.99%, Time: 31.17 seconds\n",
      "Epoch [16/30], Loss: 3.8800, Test Acc: 12.85%, Time: 31.25 seconds\n",
      "Epoch [17/30], Loss: 3.8159, Test Acc: 12.79%, Time: 31.15 seconds\n",
      "Epoch [18/30], Loss: 3.7395, Test Acc: 14.24%, Time: 31.17 seconds\n",
      "Epoch [19/30], Loss: 3.6569, Test Acc: 14.65%, Time: 31.21 seconds\n",
      "Epoch [20/30], Loss: 3.5716, Test Acc: 16.84%, Time: 31.12 seconds\n",
      "Epoch [21/30], Loss: 3.4930, Test Acc: 17.63%, Time: 31.11 seconds\n",
      "Epoch [22/30], Loss: 3.4107, Test Acc: 18.76%, Time: 31.22 seconds\n",
      "Epoch [23/30], Loss: 3.3343, Test Acc: 19.62%, Time: 31.09 seconds\n",
      "Epoch [24/30], Loss: 3.2585, Test Acc: 20.34%, Time: 31.09 seconds\n",
      "Epoch [25/30], Loss: 3.1863, Test Acc: 21.12%, Time: 31.24 seconds\n",
      "Epoch [26/30], Loss: 3.1142, Test Acc: 21.48%, Time: 31.98 seconds\n",
      "Epoch [27/30], Loss: 3.0478, Test Acc: 23.11%, Time: 31.62 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m optimizer = BayesianOptimization(\n\u001b[32m      2\u001b[39m     f=optimize_kan,\n\u001b[32m      3\u001b[39m     pbounds=pbounds,\n\u001b[32m      4\u001b[39m     random_state=\u001b[32m38\u001b[39m,\n\u001b[32m      5\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 best combo so far →\u001b[39m\u001b[33m\"\u001b[39m, optimizer.max)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:338\u001b[39m, in \u001b[36mBayesianOptimization.maximize\u001b[39m\u001b[34m(self, init_points, n_iter)\u001b[39m\n\u001b[32m    336\u001b[39m     x_probe = \u001b[38;5;28mself\u001b[39m.suggest()\n\u001b[32m    337\u001b[39m     iteration += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration > \u001b[32m0\u001b[39m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_bounds(\u001b[38;5;28mself\u001b[39m._bounds_transformer.transform(\u001b[38;5;28mself\u001b[39m._space))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:270\u001b[39m, in \u001b[36mBayesianOptimization.probe\u001b[39m\u001b[34m(self, params, lazy)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m._queue.append(params)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch(Events.OPTIMIZATION_STEP)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/bayes_opt/target_space.py:418\u001b[39m, in \u001b[36mTargetSpace.probe\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    416\u001b[39m     error_msg = \u001b[33m\"\u001b[39m\u001b[33mNo target function has been provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m.register(x, target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36moptimize_kan\u001b[39m\u001b[34m(epochs, kan_1, kan_2, kan_3, spline_cp, spline_deg, range_min, range_max, lr)\u001b[39m\n\u001b[32m     89\u001b[39m lr          = \u001b[38;5;28mfloat\u001b[39m(lr)\n\u001b[32m     91\u001b[39m model = KANCNN(\n\u001b[32m     92\u001b[39m     kan_1=kan_1,\n\u001b[32m     93\u001b[39m     kan_2=kan_2,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     range_max=range_max\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m val_acc = \u001b[43mtrain_kan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# BayesOpt maximizes the returned value\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m val_acc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_kan\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, lr)\u001b[39m\n\u001b[32m     31\u001b[39m logits = model(x)\n\u001b[32m     32\u001b[39m loss   = criterion(logits, y)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m optim.step()\n\u001b[32m     36\u001b[39m acc = (logits.argmax(\u001b[32m1\u001b[39m) == y).float().mean().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLsp2025_project/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_kan,\n",
    "    pbounds=pbounds,\n",
    "    random_state=38,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=2, n_iter=20)\n",
    "\n",
    "print(\"🚀 best combo so far →\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
