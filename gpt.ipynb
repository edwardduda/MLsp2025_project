{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, Union, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose, Resize, RandomHorizontalFlip, ToImage,\n",
    "    ToDtype, Normalize, Lambda                    # ⬅ new\n",
    ")\n",
    "from torchvision.utils import make_grid\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# -----------------------  constants  ---------------------------------\n",
    "SEED               = 42\n",
    "N_EPOCHS           = 10\n",
    "BATCH_SIZE         = 128\n",
    "IMAGE_SIZE         = (64, 64)            # Tiny‑ImageNet native size\n",
    "LEARNING_RATE      = 3e-4\n",
    "WEIGHT_DECAY       = 1e-5\n",
    "RUN_DIR            = Path(\"runs/tinyimagenet_cnn_vs_kan\").resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"➡️  Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------  utils  -------------------------------------\n",
    "def _force_rgb(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure 3‑channel RGB.\n",
    "\n",
    "    • If RGBA ➜ drop alpha.\n",
    "    • If grayscale ➜ replicate to 3 channels.\n",
    "    \"\"\"\n",
    "    if img.shape[0] == 4:        # RGBA → RGB\n",
    "        return img[:3]\n",
    "    if img.shape[0] == 1:        # Gray → RGB by tiling\n",
    "        return img.repeat(3, 1, 1)\n",
    "    return img                   # already RGB\n",
    "\n",
    "def get_dataloaders(batch_size:int=BATCH_SIZE,\n",
    "                    image_size:Tuple[int,int]=IMAGE_SIZE,\n",
    "                    dataset_name:str=\"zh-plus/tiny-imagenet\"):\n",
    "    \"\"\"\n",
    "    Return train/val/test DataLoaders for Tiny‑ImageNet.\n",
    "\n",
    "    Splits:\n",
    "        • train  – 100 000 imgs\n",
    "        • valid  – 10 000 imgs (used here as held‑out **test**)\n",
    "    We further split 10 % of *train* into an internal validation set.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    # carve INTERNAL val from train\n",
    "    split = ds[\"train\"].train_test_split(\n",
    "        test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n",
    "    train_ds = split[\"train\"]\n",
    "    val_ds   = split[\"test\"]\n",
    "    test_ds  = ds[\"valid\"]                 # official validation → test\n",
    "\n",
    "    # ImageNet normalisation stats\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    tfms = Compose([\n",
    "        ToImage(),                       # PIL → (C,H,W) uint8 tensor\n",
    "        Lambda(_force_rgb),              # ⬅ squash grayscale → RGB\n",
    "        RandomHorizontalFlip(),\n",
    "        ToDtype(torch.float32, scale=True),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    def add_tfms(example):\n",
    "        example[\"image\"] = tfms(example[\"image\"])\n",
    "        return example\n",
    "\n",
    "    for split_ds in (train_ds, val_ds, test_ds):\n",
    "        split_ds.set_transform(add_tfms)\n",
    "\n",
    "    loader_cfg = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True,  **loader_cfg)\n",
    "    val_loader   = DataLoader(val_ds,   shuffle=False, **loader_cfg)\n",
    "    test_loader  = DataLoader(test_ds,  shuffle=False, **loader_cfg)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Tiny‑ImageNet‑sized baseline CNN\n",
    "    • Input : (B, 3, 64, 64)\n",
    "    • Output: (B, num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple[int, int, int] = (3, 64, 64),\n",
    "        num_classes: int = 200,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "\n",
    "        # ─────────── feature extractor ───────────\n",
    "        self.features = nn.Sequential(\n",
    "            # 64×64 → 64×64\n",
    "            nn.Conv2d(C_in, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            # 64×64 → 32×32\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 32×32 → 16×16\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 16×16 → 8×8\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 8×8 → 4×4\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 4×4 → 1×1 (global pooling)\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "        )\n",
    "\n",
    "        # ─────────── classifier ───────────\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),               # (B, 512, 1, 1) → (B, 512)\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight‑init\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # ────────────────────────────────────────────\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images : torch.Size([128, 3, 64, 64]) torch.float32 -2.1179039478302 255.0\n",
      "labels : tensor([183,  22, 183,  72,  61,  96, 151, 120, 195, 164,   5, 143, 116, 146,\n",
      "        135, 197, 170, 132,  48,  16]) torch.int64\n",
      "# unique classes in batch: 93\n"
     ]
    }
   ],
   "source": [
    "batch   = next(iter(train_loader))\n",
    "images  = batch[\"image\"]           \n",
    "labels  = batch[\"label\"]          \n",
    "\n",
    "print(\"images :\", images.shape, images.dtype, images.min().item(), images.max().item())\n",
    "print(\"labels :\", labels[:20], labels.dtype)\n",
    "print(\"# unique classes in batch:\", len(torch.unique(labels)))\n",
    "\n",
    "# sanity checks you should see\n",
    "#   • dtype: torch.float32\n",
    "#   • min/max roughly -2 ↔ +2      (ImageNet mean/std normalisation)\n",
    "#   • label dtype: torch.int64\n",
    "#   • at least a handful of different label values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss_fn, optim, device=device):\n",
    "    model.train()\n",
    "    tot_loss = tot_correct = 0\n",
    "    for batch in loader:\n",
    "        x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss   = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        tot_loss    += loss.item() * x.size(0)\n",
    "        tot_correct += (logits.argmax(1) == y).sum().item()\n",
    "    n = len(loader.dataset)\n",
    "    return tot_loss/n, tot_correct/n          # avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device=device):\n",
    "    model.eval()\n",
    "    tot_loss = tot_correct = 0\n",
    "    for batch in loader:\n",
    "        x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        logits = model(x)\n",
    "        loss   = loss_fn(logits, y)\n",
    "        tot_loss    += loss.item() * x.size(0)\n",
    "        tot_correct += (logits.argmax(1) == y).sum().item()\n",
    "    n = len(loader.dataset)\n",
    "    return tot_loss/n, tot_correct/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_DROPOUT = 0.15\n",
    "FIXED_WEIGHT_DECAY = 6e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_cnn(lr, epochs,\n",
    "                   train_loader=train_loader, val_loader=val_loader):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    lr = float(lr)\n",
    "    n_epochs = int(round(epochs))\n",
    "    model = BaselineCNN(dropout=FIXED_DROPOUT).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=FIXED_WEIGHT_DECAY)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"[trial] lr={lr:.1e}, epochs={n_epochs}, dr={FIXED_DROPOUT}, wd={FIXED_WEIGHT_DECAY}\")\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        tr_loss, tr_acc = train(model, train_loader, loss_fn, optimizer)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, loss_fn)\n",
    "        print(f\"Epoch {epoch:02d} • train {tr_acc*100:.2f}% / val {va_acc*100:.2f}% | loss {tr_loss:.3f}/{va_loss:.3f}\")\n",
    "\n",
    "    print(f\"Trial complete in {(time.time() - start_time):.2f}s\")\n",
    "    return va_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    \"epochs\": (35, 55),\n",
    "    \"lr\":     (1e-4, 5e-3)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  epochs   |    lr     |\n",
      "-------------------------------------------------\n",
      "[trial] lr=4.8e-03, epochs=42, dr=0.15, wd=0.0006\n",
      "Epoch 01 • train 0.51% / val 0.87% | loss 5.322/5.219\n",
      "Epoch 02 • train 1.72% / val 1.36% | loss 5.078/5.192\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=lambda lr, epochs: train_eval_cnn(lr, epochs),\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "bo.maximize(init_points=6, n_iter=25)\n",
    "print(\"best combo →\", bo.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = bo.max[\"params\"]\n",
    "best_lr = best_params[\"lr\"]\n",
    "best_epochs = int(round(best_params[\"epochs\"]))\n",
    "\n",
    "model = BaselineCNN(dropout=FIXED_DROPOUT).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr, weight_decay=FIXED_WEIGHT_DECAY)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Retraining with best combo:\")\n",
    "for epoch in range(1, best_epochs + 1):\n",
    "    tr_loss, tr_acc = train(model, train_loader, loss_fn, optimizer)\n",
    "    va_loss, va_acc = evaluate(model, val_loader, loss_fn)\n",
    "    print(f\"Epoch {epoch:02d} • train {tr_acc*100:.2f}% / val {va_acc*100:.2f}% | loss {tr_loss:.3f}/{va_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. deps ──────────────────────────────────────────────────────────────\n",
    "# pip install bayesian-optimization if you haven’t already\n",
    "from bayes_opt import BayesianOptimization\n",
    "import torch, time, json\n",
    "from pathlib import Path\n",
    "from classes.BSplineActivation import BSplineActivation\n",
    "# ── 1. make KANCNN fully hyper‑param friendly ────────────────────────────\n",
    "class KANCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Same backbone as before but every spline & layer size is configurable,\n",
    "    so BayesOpt can mess with them.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(3, *IMAGE_SIZE),\n",
    "                 conv_channels=(64, 128),\n",
    "                 kan_1=512,\n",
    "                 kan_2=256,\n",
    "                 kan_3=200,\n",
    "                 spline_cp=7,\n",
    "                 spline_deg=2,\n",
    "                 range_min=-3.0,\n",
    "                 range_max=50.0):\n",
    "        super().__init__()\n",
    "        C_in, _, _ = input_shape\n",
    "        c1, c2 = conv_channels\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(C_in, c1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c2), nn.ReLU(inplace=True), nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flat = self.features(torch.zeros(1, *input_shape)).flatten(1).size(1)\n",
    "\n",
    "        self.kan1      = nn.Linear(flat, kan_1)\n",
    "        self.kan1_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan2      = nn.Linear(kan_1, kan_2)\n",
    "        self.kan2_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "        self.kan3      = nn.Linear(kan_2, kan_3)\n",
    "        self.kan3_act  = BSplineActivation(\n",
    "            num_control_points=spline_cp,\n",
    "            degree=spline_deg,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.features(x), 1)\n",
    "        x = self.kan1(x)\n",
    "        x = self.kan1_act(x)\n",
    "        x = self.kan2(x)\n",
    "        x = self.kan2_act(x)\n",
    "        x = self.kan3(x)\n",
    "        x = self.kan3_act(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------  training / eval helpers  ---------------------------\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    return (logits.argmax(dim=1) == targets).float().mean()\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Train {epoch:02d}\", leave=False):\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item()  * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    loss_sum = acc_sum = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        y = batch[\"label\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        acc = accuracy_from_logits(logits, y)\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        acc_sum  += acc.item() * x.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return {\"loss\": loss_sum / n, \"acc\": acc_sum / n}\n",
    "\n",
    "def run_training(model:nn.Module, name:str,\n",
    "                 train_loader:DataLoader, val_loader:DataLoader) -> Dict[str, List[float]]:\n",
    "    \"\"\"Full training loop for one model.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        tic = time.time()\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "        val_metrics   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        print(f\"Epoch {epoch:2d}/{N_EPOCHS} • \"\n",
    "              f\"train acc {train_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"val acc {val_metrics['acc']*100:5.2f}% | \"\n",
    "              f\"Δt {time.time()-tic:4.1f}s\")\n",
    "\n",
    "    torch.save(model.state_dict(), RUN_DIR/f\"{name}.pt\")\n",
    "    with open(RUN_DIR/f\"{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df:pd.DataFrame):\n",
    "    \"\"\"Plot accuracy + loss curves for both models.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_acc\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_acc\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_acc\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_acc\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Accuracy vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"accuracy_curves.png\", dpi=200)\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_train_loss\"], label=\"Baseline train\")\n",
    "    ax.plot(df[\"epoch\"], df[\"baseline_val_loss\"],   label=\"Baseline val\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_train_loss\"],      label=\"KAN train\", linestyle=\"--\")\n",
    "    ax.plot(df[\"epoch\"], df[\"kan_val_loss\"],        label=\"KAN val\",   linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Epoch\"); ax.set_ylabel(\"Cross‑entropy loss\")\n",
    "    ax.set_title(\"Tiny‑ImageNet • Loss vs. Epoch\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(RUN_DIR/\"loss_curves.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Final test evaluation\n",
    "    \n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "baseline = BaselineCNN()\n",
    "kan      = KANCNN()\n",
    "\n",
    "print(f'KAN parameters: {count_parameters(kan)}')\n",
    "print(f'Baseline parameters: {count_parameters(baseline)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"📚 Training baseline CNN …\")\n",
    "#hist_base, baseline_model = run_training(baseline, \"baseline\", train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n🌀 Training KAN‑CNN …\")\n",
    "#hist_kan  = run_training(kan,      \"KAN\",      train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#test_base = evaluate(baseline, test_loader, criterion)\n",
    "test_kan  = evaluate(kan,      test_loader, criterion)\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Test accuracy: \"\n",
    "        #f\"Baseline {test_base['acc']*100:5.2f}% | \n",
    "        f\"KAN {test_kan['acc']*100:5.2f}%\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    # ints → we pass floats in but will round later\n",
    "    \"epochs\":             (35, 55),\n",
    "    \"kan_1\":          (64, 512),   # width of first KAN layer\n",
    "    \"kan_2\":          (128, 512),   # second KAN layer\n",
    "    \"kan_3\":            (200, 200),\n",
    "    \"spline_cp\":          (6, 10),      # control points\n",
    "    \"spline_deg\":         (2, 5),      # deg ≤ cp‑1 guard enforced later\n",
    "    \"range_min\":          (-5.0, -0.5),\n",
    "    \"range_max\":          (5.0, 70.0),\n",
    "    \"lr\":                 (1e-4, 5e-3)\n",
    "}\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_kan(model, train_loader, val_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tic = time.time()\n",
    "        # ─ train ─\n",
    "        model.train()\n",
    "        loss_sum = acc_sum = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            acc = (logits.argmax(1) == y).float().mean().item()\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            acc_sum  += acc * x.size(0)\n",
    "\n",
    "        # ─ eval ─\n",
    "        model.eval()\n",
    "        loss_sum_val = acc_sum_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "                logits = model(x)\n",
    "                loss   = criterion(logits, y)\n",
    "                acc    = (logits.argmax(1) == y).float().mean().item()\n",
    "                loss_sum_val += loss.item() * x.size(0)\n",
    "                acc_sum_val  += acc * x.size(0)\n",
    "\n",
    "        train_loss = loss_sum     / len(train_loader.dataset)\n",
    "        train_acc  = acc_sum      / len(train_loader.dataset)\n",
    "        val_loss   = loss_sum_val / len(val_loader.dataset)\n",
    "        val_acc    = acc_sum_val  / len(val_loader.dataset)\n",
    "        elapsed    = time.time() - tic\n",
    "\n",
    "        # ← print exactly like you had it\n",
    "        print(f\"Epoch [{ep}/{epochs}], \"\n",
    "              f\"Loss: {train_loss:.4f}, \"\n",
    "              f\"Test Acc: {val_acc*100:5.2f}%, \"\n",
    "              f\"Time: {elapsed:5.2f} seconds\")\n",
    "\n",
    "        best_val = max(best_val, val_acc)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "\n",
    "def optimize_kan(epochs,\n",
    "                 kan_inner,\n",
    "                 kan_outer,\n",
    "                 spline_cp,\n",
    "                 spline_deg,\n",
    "                 range_min,\n",
    "                 range_max,\n",
    "                 lr):\n",
    "\n",
    "    # ─ cast + sanity ─\n",
    "    epochs      = int(round(epochs))\n",
    "    kan_inner   = int(round(kan_inner))\n",
    "    kan_outer   = int(round(kan_outer))\n",
    "    spline_cp   = int(round(spline_cp))\n",
    "    spline_deg  = int(round(spline_deg))\n",
    "\n",
    "    # keep B‑spline well‑formed\n",
    "    spline_deg  = max(2, min(spline_deg, spline_cp - 1))\n",
    "    lr          = float(lr)\n",
    "\n",
    "    model = KANCNN(\n",
    "        kan_inner=kan_inner,\n",
    "        kan_outer=kan_outer,\n",
    "        spline_cp=spline_cp,\n",
    "        spline_deg=spline_deg,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max\n",
    "    )\n",
    "\n",
    "    val_acc = train_kan(model, train_loader, val_loader, epochs, lr)\n",
    "\n",
    "    # BayesOpt maximizes the returned value\n",
    "    return val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_kan,\n",
    "    pbounds=pbounds,\n",
    "    random_state=38,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 8 random warm‑ups + 10 guided iterations\n",
    "optimizer.maximize(init_points=6, n_iter=25)\n",
    "\n",
    "print(\"🚀 best combo so far →\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
